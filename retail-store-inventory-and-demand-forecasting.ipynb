{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11895299,"sourceType":"datasetVersion","datasetId":7386309}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"dc583727-45d7-4674-8db3-6e8c84a0f214","_cell_guid":"9dbc98da-72a6-4196-a952-78629bc806ce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:37.811281Z","iopub.execute_input":"2025-07-14T13:40:37.811562Z","iopub.status.idle":"2025-07-14T13:40:37.818138Z","shell.execute_reply.started":"2025-07-14T13:40:37.811543Z","shell.execute_reply":"2025-07-14T13:40:37.817125Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Import & Cleaning Hypotheses**\n\n## **1. Import-Time Auto-Corrections**  \n*(Automatically handled during data loading)*  \n\n| **Issue Type**               | **Correction Applied**          | **Preserved Rows** | **Impact Analysis**                   |\n|-------------------------------|----------------------------------|--------------------|---------------------------------------|\n| Negative Inventory            | Clip to 0                       | Yes                | May mask true stockouts               |\n| Sales > Inventory             | Cap at Inventory Level          | Yes                | Preserves volume but loses overflow   |\n| Invalid Dates                 | Drop rows                       | No                 | Creates time-series gaps              |\n| Numeric Overflow              | Downcast to unsigned            | Yes                | Reduces memory usage                  |\n\n**Hypothesis**:  \n> \"Automated corrections during import will preserve 98%+ of rows while creating traceable artifacts for analysis.\"\n\n---\n\n## **2. Post-Import Quality Checks**  \n*(Require manual validation)*  \n\n| **Check Type**          | **Detection Method**             | **Potential Actions**                  |\n|-------------------------|-----------------------------------|----------------------------------------|\n| Label Consistency       | Fuzzy matching on text fields    | Standardize categories/regions         |\n| Temporal Gaps           | Missing date detection           | Interpolate or flag as special events  |\n| Price Inflation         | 3σ deviation from product history| Verify true inflation vs data errors   |\n| Promotion Efficacy      | Lift analysis (promo vs non-promo)| Filter phantom demand from stockouts   |\n\n**Hypothesis**:  \n> \"5-15% of rows will require post-import adjustments, mainly in categorical labels and promotional periods.\"\n\n---\n\n## **3. Feature Engineering Implications**\n\n| **Observed Pattern**       | **Feature Design**               | **Rationale**                         |\n|----------------------------|-----------------------------------|---------------------------------------|\n| Frequent zero-demand       | Demand presence indicator        | Supports intermittent-demand models   |\n| Epidemic spikes            | Shock absorption features        | Isolates exceptional events           |\n| Regional price variance    | Region-price clusters            | Captures local market dynamics        |\n\n**Hypothesis**:  \n> \"Auto-corrected inventory/sales relationships will require demand reconstruction features.\"","metadata":{"_uuid":"2ca1c5d9-e4fb-4486-9fdc-fe5fadaaf53b","_cell_guid":"8ed33576-8121-4f61-91df-4866dfbb6b09","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"0be44699-ba8c-47e6-b3c6-5f50bc1b56eb","_cell_guid":"f67ae607-8ed0-417d-9270-9076007c7bad","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step A.1: Data Import and Proactive Cleaning\n# Purpose: To import the retail sales data in chunks while applying automated corrections for common data\n#          integrity issues and tracking all modifications, as outlined in the initial hypotheses.\n# Outputs: A cleaned DataFrame, a log of any import errors, and detailed statistics on dropped or modified rows.\n# Rationale: This foundational step ensures that the raw data is loaded efficiently and that baseline\n#            data quality is enforced from the start, providing a clean and well-documented dataset for analysis.\n# ==========================================================================\n\n# Imports and Configuration\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nDATA_PATH = Path('/kaggle/input/retail-store-inventory-and-demand-forecasting/sales_data.csv')\nCHUNKSIZE = 50_000\nDATE_COL = 'Date'\n\n# Optimized dtypes\nDTYPES = {\n    'Store ID': 'category',\n    'Product ID': 'category',\n    'Category': 'category',\n    'Region': 'category',\n    'Inventory Level': 'uint16',\n    'Units Sold': 'uint16',\n    'Units Ordered': 'uint16',\n    'Price': 'float32',\n    'Discount': 'float32',\n    'Weather Condition': 'category',\n    'Promotion': 'bool',\n    'Competitor Pricing': 'float32',\n    'Seasonality': 'category',\n    'Epidemic': 'bool',\n    'Demand': 'uint16'\n}\n\ndef import_retail_data():\n    \"\"\"\n    Imports retail sales data in chunks, applies corrections, and tracks changes.\n    Returns cleaned data, error messages, and processing statistics.\n    \"\"\"\n    chunks = []\n    error_log = []\n    stats = {\n        'total_rows': 0,\n        'rows_dropped': {\n            'date_parsing': 0,\n            'dtype_conversion': 0,\n            'chunk_failures': 0\n        },\n        'rows_modified': {\n            'negative_inventory': 0,\n            'sales_exceeding_inventory': 0\n        }\n    }\n\n    try:\n        with pd.read_csv(\n            DATA_PATH,\n            chunksize=CHUNKSIZE,\n            dtype=DTYPES,\n            parse_dates=[DATE_COL],\n            on_bad_lines='warn',\n            encoding='utf-8'\n        ) as reader:\n            for chunk_idx, chunk in enumerate(reader):\n                stats['total_rows'] += len(chunk)\n                try:\n                    # Validate required columns\n                    missing_cols = set(DTYPES.keys()) - set(chunk.columns)\n                    if missing_cols:\n                        raise ValueError(f\"Missing columns: {missing_cols}\")\n\n                    # Track modifications\n                    stats['rows_modified']['negative_inventory'] += (chunk['Inventory Level'] < 0).sum()\n                    stats['rows_modified']['sales_exceeding_inventory'] += (chunk['Units Sold'] > chunk['Inventory Level']).sum()\n\n                    # Clean data - Date parsing\n                    chunk[DATE_COL] = pd.to_datetime(chunk[DATE_COL], errors='coerce')\n                    date_mask = chunk[DATE_COL].isna()\n                    stats['rows_dropped']['date_parsing'] += date_mask.sum()\n                    chunk = chunk[~date_mask]\n\n                    # Clean data - Inventory adjustment\n                    chunk['Inventory Level'] = chunk['Inventory Level'].clip(lower=0)\n\n                    # Clean data - Sales adjustment\n                    chunk['Units Sold'] = chunk[['Units Sold', 'Inventory Level']].min(axis=1)\n\n                    chunks.append(chunk)\n\n                except Exception as e:\n                    stats['rows_dropped']['chunk_failures'] += len(chunk)\n                    error_log.append(f\"Chunk {chunk_idx} failed: {str(e)}\")\n                    continue\n\n        if not chunks:\n            raise ValueError(\"No valid data chunks were processed\")\n\n        df = pd.concat(chunks, ignore_index=False).set_index(DATE_COL).sort_index()\n        stats['final_rows'] = len(df)\n\n        return df, error_log, stats\n\n    except Exception as e:\n        error_log.append(f\"Fatal import error: {str(e)}\")\n        return None, error_log, stats\n\n# Execute import and log errors\ndf, import_errors, import_stats = import_retail_data()","metadata":{"_uuid":"cfee8bb9-b070-434a-bbc0-5fa345d80a0d","_cell_guid":"1225e8b8-6c09-4f3f-be2a-4c32265fa51a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:37.819508Z","iopub.execute_input":"2025-07-14T13:40:37.819857Z","iopub.status.idle":"2025-07-14T13:40:38.363027Z","shell.execute_reply.started":"2025-07-14T13:40:37.819798Z","shell.execute_reply":"2025-07-14T13:40:38.361886Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================================\n# Step A.2: Report on Initial Data Import and Cleaning\n# Purpose: To display the results from the proactive data import, showing the final row count, a summary of all\n#          automated corrections and dropped rows, and a sample of the resulting DataFrame.\n# Outputs: Printed statements summarizing the import process and a display of the head of the cleaned DataFrame.\n# Rationale: This step provides immediate feedback on the success and impact of the initial data import and\n#            cleaning phase, offering a transparent view of what changes were made automatically.\n# ==========================================================================\n\n\n\nif df is not None:\n    print(f\"Successfully imported {import_stats['final_rows']:,} rows (out of {import_stats['total_rows']:,} total)\")\n    \n    print(\"\\nRows dropped during processing:\")\n    for reason, count in import_stats['rows_dropped'].items():\n        print(f\"- {reason.replace('_', ' ').title()}: {count:,} rows\")\n    \n    print(\"\\nRows auto-corrected (not dropped):\")\n    for issue, count in import_stats['rows_modified'].items():\n        print(f\"- {issue.replace('_', ' ').title()}: {count:,} rows\")\n    \n    print(\"\\nSample data:\")\n    display(df.head())\nelse:\n    print(\"Import failed\")\n\nif import_errors:\n    print(\"\\nEncountered warnings/errors:\")\n    for i, error in enumerate(import_errors, 1):\n        print(f\"{i}. {error}\")\nelse:\n    print(\"\\nNo import errors detected.\")","metadata":{"_uuid":"c2c35946-51b6-4ebb-806b-1da9c01240bb","_cell_guid":"b4f2be12-4840-4e92-8595-309c69e87848","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:38.364509Z","iopub.execute_input":"2025-07-14T13:40:38.364952Z","iopub.status.idle":"2025-07-14T13:40:38.385421Z","shell.execute_reply.started":"2025-07-14T13:40:38.364927Z","shell.execute_reply":"2025-07-14T13:40:38.384303Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"_uuid":"a43fe5c2-d11a-4d7a-8078-ab5c59b9ed69","_cell_guid":"ec9c6a57-4dc0-47b3-ac94-2d33380d90e5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Retail Data Quality Assessment Framework\n\n---\n\n## 1. Investigation Purpose\n\nSystematically evaluate dataset quality to:\n\n- **Identify integrity issues affecting forecast accuracy**\n- **Uncover demand, sales, inventory, and regional patterns**\n- **Guide model selection based on data characteristics**\n- **Document critical metadata for reproducibility**\n\n---\n\n## 2. Key Investigation Areas\n\n### A. Data Quality Checks\n\n**Rationale:**  \nEnsure reliable inputs for forecasting.\n\n**Targets:**\n\n1. **Null/NaN and missing data:**  \n   Quantify and address gaps in key columns.\n2. **Row completeness:**  \n   Ensure all required columns are populated.\n3. **Data type verification:**  \n   Confirm correct types for each column.\n4. **Unique identifiers:**  \n   Check for duplicates in key fields.\n5. **Category/label consistency:**  \n   Detect typos, inconsistencies, and rare/catch-all labels.\n6. **Date validity:**  \n   Identify gaps or anomalous timestamps.\n7. **Value validation:**  \n   Flag artificial caps, placeholder values, or illogical relationships (e.g., sales > inventory, negative prices).\n8. **Flag consistency:**  \n   Validate binary flags (promotion, epidemic).\n\n**Outputs:**\n\n- **Counts of missing/incomplete/invalid values**\n- **Visualizations:** Bar plots, heatmaps, histograms\n\n---\n\n\n### B. Dataset Dimensions\n\n**Rationale:** Understand data granularity and coverage  \n**Metrics:**\n\n| Dimension         | Purpose                                                         |\n|-------------------|-----------------------------------------------------------------|\n| Product Count     | Determine SKU-level vs. category forecasting                    |\n| Category Count    | Identify overbroad or inconsistent groupings                    |\n| Date Range        | Assess seasonality coverage and temporal completeness           |\n| Store Count       | Evaluate store-level completeness and granularity               |\n| Region Count      | Assess geographical representation and region-level consistency |\n\n---\n\n### C. Demand Characteristics\n\n**Rationale:** Inform model selection  \n**Analysis:**\n\n1. **Value range (min/max):** Detect censoring or artificial limits\n2. **Zero-demand frequency:** Identify intermittent demand patterns\n3. **Epidemic/promotion variance:** Assess shock resilience and event-driven demand\n4. **Regional patterns:** Detect variance in sales/demand across regions\n\n**Outputs:**\n\n- **Summary statistics**\n- **Distribution plots, time series with event markers, regional heatmaps**\n\n---\n\n## 3. Insights-to-Action Mapping\n\n| Finding                | Impact                              | Action                                      |\n|------------------------|-------------------------------------|---------------------------------------------|\n| High zero-demand %     | Intermittent demand pattern         | Use Croston’s/TSB models                    |\n| Regional sales clusters| Geographic bias                     | Add region dummy variables                  |\n| Epidemic demand spikes | Non-stationary time series          | Implement shock indicator features          |\n| Category/label typos   | Misattribution, inconsistent analysis| Consolidate labels pre-analysis             |\n| Missing weather/pricing| Incomplete contextual factors       | Impute regionally or exclude weather-dependent models |\n| Value anomalies        | Implausible results                 | Correct or flag for review                  |\n| Flag inconsistencies   | Misleading uplift analysis          | Correct/validate flags                      |\n| Negative inventory     | Implausible sales patterns          | Correct or flag for review                  |\n\n---\n\n## 4. Target Outputs\n\n- **Data Quality Scorecard**\n  - Summary metrics for each quality dimension\n  - Visual dashboards of key findings\n- **Priority Cleaning Tasks**\n  - Roadmap for addressing identified issues\n- **Model Compatibility Assessment**\n  - Evaluation of data suitability for different forecasting models\n- **Feature Engineering Roadmap**\n  - Plan for creating derived features (e.g., region dummies, event flags)\n- **Model Selection Justification**\n  - Documentation of model choices based on data characteristics\n\n---","metadata":{"_uuid":"0753c997-9449-4b5a-9762-c1f7be68e79c","_cell_guid":"7f8ab8fb-ed28-41a1-b9ff-82f2c0fe67ac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-08T09:13:41.085934Z","iopub.execute_input":"2025-06-08T09:13:41.086226Z","iopub.status.idle":"2025-06-08T09:13:41.106711Z","shell.execute_reply.started":"2025-06-08T09:13:41.086206Z","shell.execute_reply":"2025-06-08T09:13:41.105401Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step A: Foundational Data Quality Checks\n# Purpose: To perform a comprehensive first-pass analysis of the dataset, identifying common quality issues\n#          like missing values, incorrect data types, duplicates, invalid values, and inconsistent flags.\n# Outputs: A consolidated report, visualizations for each check, and a dictionary of problematic row indices.\n# Rationale: Establishing a baseline of data quality is critical before deeper analysis or modeling,\n#            ensuring subsequent steps are built on a reliable foundation.\n# ==========================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ================================================================\n# Reset index for checks: Make 'Date' a column for validation\n# ================================================================\ndf_check = df.reset_index()\n\n# Initialize dictionary to store problematic row indices\nproblem_rows = {\n    'missing_any': set(),\n    'missing_required': set(),\n    'duplicate_composite': set(),\n    'invalid_date': set(),\n    'future_date': set(),\n    'ancient_date': set(),\n    'negative_inventory': set(),\n    'negative_price': set(),\n    'sales_exceeds_inventory': set(),\n    'non_binary_promotion': set(),\n    'non_binary_epidemic': set(),\n    'rare_category': set(),\n    'rare_region': set(),\n    'rare_weather': set(),\n    'rare_seasonality': set(),\n}\n\n# Initialize a string to collect all report text\nreport_text = \"\"\n\ndef log_report(message):\n    \"\"\"Helper function to accumulate report text.\"\"\"\n    global report_text\n    report_text += message + \"\\n\"\n\n# -----------------------------------------------------------------\n# 1. Null/NaN and Missing Data\n# -----------------------------------------------------------------\ntry:\n    missing_any = df_check[df_check.isna().any(axis=1)].index\n    problem_rows['missing_any'].update(missing_any)\n    log_report(f\"Rows with any missing data: {len(missing_any)}\")\n    \n    # --- Enhanced Plotting ---\n    plt.figure(figsize=(12, 6))\n    missing_counts = df_check.isna().sum()\n    missing_counts = missing_counts[missing_counts > 0] # Only plot columns with missing values\n    if not missing_counts.empty:\n        ax = missing_counts.plot(kind='bar', color='salmon', alpha=0.8)\n        \n        # Add data labels\n        for p in ax.patches:\n            ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                        ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=10)\n\n        plt.title(\"Count of Missing Values by Column (Before Cleaning)\", fontsize=16)\n        plt.ylabel(\"Number of Missing Rows\", fontsize=12)\n        plt.xlabel(\"Column Name\", fontsize=12)\n        plt.xticks(rotation=45, ha='right')\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n    else:\n        log_report(\"No missing values found in any column.\")\n\nexcept Exception as e:\n    log_report(f\"Error checking for missing data: {e}\")\n\n# -----------------------------------------------------------------\n# 2. Row Completeness\n# -----------------------------------------------------------------\nrequired_cols = ['Store ID', 'Product ID', 'Date', 'Units Sold']  # Adjust as needed\ntry:\n    missing_required = df_check[df_check[required_cols].isna().any(axis=1)].index\n    problem_rows['missing_required'].update(missing_required)\n    log_report(f\"Rows missing required columns: {len(missing_required)}\")\n\n    # --- Enhanced Plotting ---\n    plt.figure(figsize=(10, 5))\n    missing_required_counts = df_check[required_cols].isna().sum()\n    missing_required_counts = missing_required_counts[missing_required_counts > 0]\n    if not missing_required_counts.empty:\n        ax = missing_required_counts.plot(kind='bar', color='skyblue', alpha=0.8)\n\n        # Add data labels\n        for p in ax.patches:\n            ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                        ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=10)\n\n        plt.title(\"Missing Values in Required Columns\", fontsize=16)\n        plt.ylabel(\"Number of Missing Rows\", fontsize=12)\n        plt.xlabel(\"Column Name\", fontsize=12)\n        plt.xticks(rotation=45, ha='right')\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n    else:\n        log_report(\"No missing values found in required columns.\")\n\nexcept Exception as e:\n    log_report(f\"Error checking for missing required columns: {e}\")\n\n# -----------------------------------------------------------------\n# 3. Data Type Verification\n# -----------------------------------------------------------------\ntry:\n    log_report(\"Data Types:\\n\" + str(df_check.dtypes))\n    \n    # --- Enhanced Plotting ---\n    plt.figure(figsize=(10, 5))\n    dtype_counts = df_check.dtypes.value_counts()\n    if not dtype_counts.empty:\n        ax = dtype_counts.plot(kind='bar', color='limegreen', alpha=0.8)\n\n        # Add data labels\n        for p in ax.patches:\n            ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                        ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=10)\n\n        plt.title(\"Distribution of Column Data Types After Import\", fontsize=16)\n        plt.ylabel(\"Number of Columns\", fontsize=12)\n        plt.xlabel(\"Data Type\", fontsize=12)\n        plt.xticks(rotation=0)\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n    else:\n        log_report(\"Could not determine data types.\")\n\nexcept Exception as e:\n    log_report(f\"Error checking data types: {e}\")\n\n# -----------------------------------------------------------------\n# 4. Unique Identifiers (Composite Key)\n# -----------------------------------------------------------------\ntry:\n    duplicate_composite = df_check[df_check.duplicated(subset=['Store ID', 'Product ID', 'Date'], keep=False)].index\n    problem_rows['duplicate_composite'].update(duplicate_composite)\n    log_report(f\"Duplicate records by composite key: {len(duplicate_composite)}\")\nexcept Exception as e:\n    log_report(f\"Error checking for duplicate rows: {e}\")\n\n# -----------------------------------------------------------------\n# 5. Category/Label Consistency\n# -----------------------------------------------------------------\ncat_cols = {\n    'Category': 'rare_category',\n    'Region': 'rare_region',\n    'Weather Condition': 'rare_weather',\n    'Seasonality': 'rare_seasonality',\n}\nthreshold = 5  # Adjust as needed\ntry:\n    for col, key in cat_cols.items():\n        # Logic for finding rare labels remains the same\n        value_counts = df_check[col].value_counts()\n        rare_labels = value_counts[value_counts < threshold].index\n        rare_rows = df_check[df_check[col].isin(rare_labels)].index\n        problem_rows[key].update(rare_rows)\n        log_report(f\"Rows with rare '{col}' labels (count < {threshold}): {len(rare_rows)}\")\n\n        # --- Enhanced Plotting ---\n        plt.figure(figsize=(12, 6))\n        \n        # For clarity, only plot top 20 most frequent labels\n        plot_counts = value_counts.head(20)\n        \n        ax = plot_counts.plot(kind='bar', color='purple', alpha=0.8)\n\n        # Add data labels\n        for p in ax.patches:\n            ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                        ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=9)\n\n        title_suffix = \"\"\n        if len(value_counts) > 20:\n            title_suffix = \" (Top 20 Shown)\"\n\n        plt.title(f\"Distribution of '{col}' Labels{title_suffix}\", fontsize=16)\n        plt.ylabel(\"Number of Records\", fontsize=12)\n        plt.xlabel(f\"{col} Label\", fontsize=12)\n        plt.xticks(rotation=45, ha='right')\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n\nexcept Exception as e:\n    log_report(f\"Error checking category/label consistency: {e}\")\n\n# -----------------------------------------------------------------\n# 6. Date Validity\n# -----------------------------------------------------------------\ntry:\n    # Invalid dates (NaT after parsing)\n    invalid_date = df_check[df_check['Date'].isna()].index\n    problem_rows['invalid_date'].update(invalid_date)\n    log_report(f\"Rows with invalid dates: {len(invalid_date)}\")\n    \n    # Future dates\n    today = pd.Timestamp.today().normalize()\n    future_date = df_check[df_check['Date'] > today].index\n    problem_rows['future_date'].update(future_date)\n    log_report(f\"Rows with future dates: {len(future_date)}\")\n    \n    # Ancient dates (adjust threshold as needed)\n    ancient_date = df_check[df_check['Date'] < pd.Timestamp('2000-01-01')].index\n    problem_rows['ancient_date'].update(ancient_date)\n    log_report(f\"Rows with ancient dates: {len(ancient_date)}\")\n\n    # --- Enhanced Plotting: Date Coverage Heatmap ---\n    if 'Date' in df_check.columns and not df_check['Date'].isna().all():\n        # Create a pivot table for the heatmap\n        df_check['Year'] = df_check['Date'].dt.year\n        df_check['Month'] = df_check['Date'].dt.month\n        date_heatmap = df_check.pivot_table(index='Year', columns='Month', values='Store ID', aggfunc='count').fillna(0)\n\n        plt.figure(figsize=(14, 7))\n        sns.heatmap(date_heatmap, cmap='viridis', annot=True, fmt='.0f', linewidths=.5)\n        \n        plt.title(\"Monthly Transaction Heatmap\", fontsize=16)\n        plt.xlabel(\"Month\", fontsize=12)\n        plt.ylabel(\"Year\", fontsize=12)\n        plt.tight_layout()\n        plt.show()\n\n        # Clean up temporary columns\n        df_check = df_check.drop(columns=['Year', 'Month'])\n    else:\n        log_report(\"Skipping date heatmap due to missing or invalid date column.\")\n\nexcept Exception as e:\n    log_report(f\"Error checking date validity: {e}\")\n\n# -----------------------------------------------------------------\n# 7. Value Validation\n# -----------------------------------------------------------------\ntry:\n    negative_inventory = df_check[df_check['Inventory Level'] < 0].index\n    problem_rows['negative_inventory'].update(negative_inventory)\n    log_report(f\"Rows with negative inventory: {len(negative_inventory)}\")\n    \n    negative_price = df_check[df_check['Price'] < 0].index\n    problem_rows['negative_price'].update(negative_price)\n    log_report(f\"Rows with negative price: {len(negative_price)}\")\n    \n    sales_exceeds_inventory = df_check[df_check['Units Sold'] > df_check['Inventory Level']].index\n    problem_rows['sales_exceeds_inventory'].update(sales_exceeds_inventory)\n    log_report(f\"Rows where sales exceed inventory: {len(sales_exceeds_inventory)}\")\n\n    # --- Enhanced Plotting ---\n    value_issues = {\n        'Negative Inventory': len(negative_inventory),\n        'Negative Price': len(negative_price),\n        'Sales > Inventory': len(sales_exceeds_inventory)\n    }\n    value_issues_series = pd.Series(value_issues)\n    value_issues_series = value_issues_series[value_issues_series > 0]\n\n    if not value_issues_series.empty:\n        plt.figure(figsize=(10, 5))\n        ax = value_issues_series.plot(kind='bar', color='orange', alpha=0.8)\n\n        # Add data labels\n        for p in ax.patches:\n            ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                        ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=10)\n\n        plt.title(\"Count of Rows Failing Initial Value Validation\", fontsize=16)\n        plt.ylabel(\"Number of Rows\", fontsize=12)\n        plt.xlabel(\"Validation Check\", fontsize=12)\n        plt.xticks(rotation=0)\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n    else:\n        log_report(\"No value validation issues found.\")\n\nexcept Exception as e:\n    log_report(f\"Error checking value validation: {e}\")\n\n# -----------------------------------------------------------------\n# 8. Flag Consistency\n# -----------------------------------------------------------------\ntry:\n    non_binary_promotion = df_check[~df_check['Promotion'].isin([True, False, 0, 1])].index\n    problem_rows['non_binary_promotion'].update(non_binary_promotion)\n    log_report(f\"Rows with non-binary Promotion: {len(non_binary_promotion)}\")\n    \n    non_binary_epidemic = df_check[~df_check['Epidemic'].isin([True, False, 0, 1])].index\n    problem_rows['non_binary_epidemic'].update(non_binary_epidemic)\n    log_report(f\"Rows with non-binary Epidemic: {len(non_binary_epidemic)}\")\n\n    # --- Enhanced Plotting ---\n    for flag in ['Promotion', 'Epidemic']:\n        plt.figure(figsize=(8, 4))\n        flag_counts = df_check[flag].value_counts()\n        \n        ax = flag_counts.plot(kind='bar', color='cornflowerblue', alpha=0.8)\n\n        # Add data labels\n        for p in ax.patches:\n            ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                        ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=10)\n\n        plt.title(f\"Distribution of '{flag}' Flag\", fontsize=16)\n        plt.ylabel(\"Number of Records\", fontsize=12)\n        plt.xlabel(\"Flag Value\", fontsize=12)\n        plt.xticks(rotation=0)\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n\nexcept Exception as e:\n    log_report(f\"Error checking flag consistency: {e}\")\n\n# -----------------------------------------------------------------\n# 9. Final Consolidated Report\n# -----------------------------------------------------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"A. Data Quality Checks: Consolidated Report\")\nprint(\"=\"*80)\nprint(report_text)\nprint(\"All problematic row indices have been stored in the 'problem_rows' dictionary.\")\n\n# ================================================================\n# Restore index for time series operations (optional)\n# ================================================================\ndf = df_check.set_index('Date').sort_index()","metadata":{"_uuid":"8d91ffeb-19d5-408d-9197-4ca6966e8583","_cell_guid":"bc776d5f-817c-4a78-a477-f956957af298","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:38.386492Z","iopub.execute_input":"2025-07-14T13:40:38.386782Z","iopub.status.idle":"2025-07-14T13:40:40.451770Z","shell.execute_reply.started":"2025-07-14T13:40:38.386751Z","shell.execute_reply":"2025-07-14T13:40:40.450870Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"_uuid":"447003fa-b6b0-40fa-8d53-ffd1d0f8f8e9","_cell_guid":"8557c004-ae46-4368-89cd-1ab598db8044","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step B.1: Granularity, Coverage, and Data Integrity Checks\n# Purpose: To ensure the dataset is complete, consistent, and fit for forecasting by analyzing its\n#          granularity, coverage, and identifying critical integrity issues like duplicates and value anomalies.\n# Outputs: A printed audit log, a dictionary of problematic rows, and summary visualizations.\n# Rationale: This transparent, progressive check supports actionable data cleaning by systematically\n#            validating the core dimensions and integrity of the data before advanced analysis.\n# ==========================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assume df is your DataFrame, with 'Date' as index and columns:\n# 'Product ID', 'Category', 'Store ID', 'Region', 'Inventory Level', 'Units Sold', 'Price', 'Promotion', 'Epidemic'\n\n# Initialize log as a list (to be printed at the end)\nlog = []\nlog.append(\"=== DATA VALIDATION AUDIT LOG ===\\n\\n\")\n\n# Store problematic rows as pandas Series or DataFrames\nproblematic_rows = {}\n\n# ==========================================================================\n# Define validation rules and metrics\n# ==========================================================================\nvalidation_rules = {\n    'Inventory Level': {'min': 0, 'max': 10000},\n    'Price': {'min': 0, 'max': 1000},\n    'Units Sold': {'min': 0, 'max': 1000},\n    'Promotion': {'allowed': [True, False, 1, 0]},\n    'Epidemic': {'allowed': [True, False, 1, 0]}\n}\nrequired_cols = ['Product ID', 'Category', 'Store ID', 'Region', 'Inventory Level', 'Units Sold', 'Price', 'Promotion', 'Epidemic']\n\ntry:\n    # ==========================================================================\n    # Automated data quality checks\n    # ==========================================================================\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        log.append(f\"Missing required columns: {missing_cols}\\n\")\n        problematic_rows['missing_columns'] = missing_cols\n    else:\n        log.append(\"All required columns present.\\n\")\n\n    missing_values = df[required_cols].isna().sum()\n    if missing_values.sum() > 0:\n        log.append(\"Missing values detected:\\n\")\n        log.append(missing_values.to_string() + \"\\n\")\n        problematic_rows['missing_values'] = missing_values\n    else:\n        log.append(\"No missing values found in required columns.\\n\")\n\n    # ==========================================================================\n    # Change index to column for duplicate check on composite key\n    # This creates a temporary DataFrame with 'Date' as a column, not the index\n    # ==========================================================================\n    df_with_date = df.reset_index()\n    composite_key = ['Store ID', 'Product ID', 'Date']\n    duplicates = df_with_date[df_with_date.duplicated(subset=composite_key, keep=False)]\n    if not duplicates.empty:\n        log.append(f\"Duplicate rows by composite key (Store/Product/Date): {len(duplicates)}\\n\")\n        problematic_rows['duplicates'] = duplicates\n    else:\n        log.append(\"No duplicate rows found by composite key (Store/Product/Date).\\n\")\n\n    # ==========================================================================\n    # Check for possible duplicate rows by date only (if date is index)\n    # This helps identify days with suspiciously high record counts\n    # ==========================================================================\n    date_duplicates = df.groupby(df.index).size()\n    suspicious_dates = date_duplicates[date_duplicates > date_duplicates.median() + 2 * date_duplicates.std()]\n    if not suspicious_dates.empty:\n        log.append(f\"\\nPossible duplicate or suspiciously high record counts by date:\\n\")\n        log.append(f\"Found {len(suspicious_dates)} dates with unusually high record counts.\\n\")\n        log.append(\"Top dates with high record counts:\\n\")\n        log.append(suspicious_dates.sort_values(ascending=False).head().to_string() + \"\\n\")\n        problematic_rows['suspicious_dates'] = suspicious_dates\n    else:\n        log.append(\"No suspiciously high record counts by date found.\\n\")\n\n    for field, rules in validation_rules.items():\n        if field in df.columns:\n            if 'min' in rules:\n                below_min = df[df[field] < rules['min']]\n                if not below_min.empty:\n                    log.append(f\"Rows with {field} below minimum ({rules['min']}): {len(below_min)}\\n\")\n                    problematic_rows[f'{field}_below_min'] = below_min[field]\n            if 'max' in rules:\n                above_max = df[df[field] > rules['max']]\n                if not above_max.empty:\n                    log.append(f\"Rows with {field} above maximum ({rules['max']}): {len(above_max)}\\n\")\n                    problematic_rows[f'{field}_above_max'] = above_max[field]\n            if 'allowed' in rules:\n                invalid = df[~df[field].isin(rules['allowed'])]\n                if not invalid.empty:\n                    log.append(f\"Rows with invalid {field}: {len(invalid)}\\n\")\n                    problematic_rows[f'invalid_{field}'] = invalid[field]\n\n    # ==========================================================================\n    # Granularity and coverage analysis\n    # ==========================================================================\n    metrics = {\n        \"Product Count\": df['Product ID'].nunique(),\n        \"Category Count\": df['Category'].nunique(),\n        \"Date Range\": (df.index.min(), df.index.max()),\n        \"Store Count\": df['Store ID'].nunique(),\n        \"Region Count\": df['Region'].nunique()\n    }\n\n    log.append(\"\\n=== DATASET DIMENSIONS ===\\n\")\n    for key, val in metrics.items():\n        if isinstance(val, tuple):\n            log.append(f\"{key}: {val[0].date()} to {val[1].date()}\\n\")\n        else:\n            log.append(f\"{key}: {val}\\n\")\n\n    # ==========================================================================\n    # Actionable insights and remediation (context-aware, only if issues found)\n    # ==========================================================================\n    log.append(\"\\n=== ACTIONABLE INSIGHTS ===\\n\")\n    any_issues = False\n\n    if 'missing_columns' in problematic_rows:\n        log.append(\"1. Review and correct missing required columns: see 'missing_columns'.\\n\")\n        any_issues = True\n    if 'missing_values' in problematic_rows:\n        log.append(\"2. Review and correct missing values: see 'missing_values'.\\n\")\n        any_issues = True\n    if 'duplicates' in problematic_rows:\n        log.append(\"3. Investigate and resolve duplicate records: see 'duplicates'.\\n\")\n        any_issues = True\n    if 'suspicious_dates' in problematic_rows:\n        log.append(\"4. Investigate suspiciously high record counts by date: see 'suspicious_dates'.\\n\")\n        any_issues = True\n\n    # Field-specific checks\n    for field in ['Inventory Level', 'Price', 'Units Sold', 'Promotion', 'Epidemic']:\n        if f'{field}_below_min' in problematic_rows:\n            log.append(f\"5. Check and correct rows with {field} below minimum: see '{field}_below_min'.\\n\")\n            any_issues = True\n        if f'{field}_above_max' in problematic_rows:\n            log.append(f\"6. Check and correct rows with {field} above maximum: see '{field}_above_max'.\\n\")\n            any_issues = True\n        if f'invalid_{field}' in problematic_rows:\n            log.append(f\"7. Validate and standardize {field} values: see 'invalid_{field}'.\\n\")\n            any_issues = True\n\n    if not any_issues:\n        log.append(\"No data quality issues found. No corrective actions required.\\n\")\n\n    # ==========================================================================\n    # Statistical and visual analysis (print plots to notebook)\n    # ==========================================================================\n    try:\n        # --- Enhanced Daily Record Count Plot ---\n        plt.figure(figsize=(12, 6))\n        daily_counts = df.resample('D').size()\n        \n        # Plot daily counts\n        daily_counts.plot(label='Daily Record Count', color='skyblue', alpha=0.8)\n        \n        # Plot 30-day rolling average\n        daily_counts.rolling(window=30).mean().plot(label='30-Day Rolling Average', color='darkblue', linestyle='--')\n\n        plt.title('Daily Record Count and 30-Day Rolling Average', fontsize=16)\n        plt.ylabel('Number of Records', fontsize=12)\n        plt.xlabel('Date', fontsize=12)\n        plt.legend()\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n\n        # --- Enhanced Units Sold Distribution Plot ---\n        plt.figure(figsize=(12, 6))\n        sns.histplot(df['Units Sold'], kde=True, bins=50, color='lightcoral', alpha=0.8)\n        \n        # Add mean and median lines\n        mean_units_sold = df['Units Sold'].mean()\n        median_units_sold = df['Units Sold'].median()\n        plt.axvline(mean_units_sold, color='red', linestyle='--', label=f'Mean: {mean_units_sold:.2f}')\n        plt.axvline(median_units_sold, color='green', linestyle='-', label=f'Median: {median_units_sold:.2f}')\n\n        plt.title('Distribution of Units Sold (All Records)', fontsize=16)\n        plt.xlabel('Units Sold', fontsize=12)\n        plt.ylabel('Frequency', fontsize=12)\n        plt.legend()\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        log.append(f\"Error during visualization: {e}\\n\")\n\nexcept Exception as e:\n    log.append(f\"Critical error during validation: {e}\\n\")\n\n# ==========================================================================\n# Print log to cell output\n# ==========================================================================\nprint(''.join(log))\nprint(\"\\nData validation checks completed. Problematic rows are stored in the 'problematic_rows' dictionary.\")\n\n# Optional: Print problematic rows for review\nif problematic_rows:\n    print(\"\\n=== PROBLEMATIC ROWS ===\\n\")\n    for key, rows in problematic_rows.items():\n        print(f\"{key}:\")\n        print(rows if isinstance(rows, (pd.Series, pd.DataFrame)) else rows)\n        print()\nelse:\n    print(\"\\nNo problematic rows found.\")","metadata":{"_uuid":"9f00ad84-4f97-4e26-812e-888a7833d767","_cell_guid":"03b65807-72cf-4b34-9a17-0f637b690dd0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:40.453507Z","iopub.execute_input":"2025-07-14T13:40:40.453769Z","iopub.status.idle":"2025-07-14T13:40:41.534744Z","shell.execute_reply.started":"2025-07-14T13:40:40.453747Z","shell.execute_reply":"2025-07-14T13:40:41.533713Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================================\n# Step B.2: Advanced Temporal Integrity Checks\n# Purpose: To perform deeper validation of the dataset's time-series structure by checking for\n#          temporal continuity, identifying out-of-bounds timestamps, and flagging suspicious record volumes.\n# Outputs: A printed summary report, a dictionary of problematic rows, and actionable insights for remediation.\n# Rationale: High-standard temporal validation is essential for reliable time-series forecasting, as gaps\n#            or anomalies can significantly skew model performance.\n# ==========================================================================\n\nfrom datetime import datetime\n\n# Helper function to format date lists\ndef format_dates(dates):\n    if hasattr(dates, 'empty') and dates.empty:\n        return 'None'\n    elif len(dates) == 0:\n        return 'None'\n    return ', '.join([d.strftime('%Y-%m-%d') for d in dates])\n\n# 1. Date Sequence Completeness\nexpected_dates = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\nmissing_dates = expected_dates.difference(df.index)\nmissing_dates_list = list(missing_dates)\n\n# 2. Logical Timestamp Ranges\ntoday = datetime.now().date()\nfuture_dates = df.index[df.index > pd.Timestamp(today)]\nancient_threshold = pd.Timestamp('2000-01-01')  # Adjust as needed\nancient_dates = df.index[df.index < ancient_threshold]\n\n# 3. Suspiciously High or Low Record Counts by Date\ndate_counts = df.groupby(df.index).size()\nsuspicious_dates = date_counts[\n    (date_counts > date_counts.median() + 2 * date_counts.std()) |\n    (date_counts < date_counts.median() - 2 * date_counts.std())\n]\n\n# 4. Dataset Dimension Recap (for reporting)\ndimension_metrics = {\n    \"Product Count\": df['Product ID'].nunique(),\n    \"Category Count\": df['Category'].nunique(),\n    \"Date Range\": (df.index.min().date(), df.index.max().date()),\n    \"Store Count\": df['Store ID'].nunique(),\n    \"Region Count\": df['Region'].nunique()\n}\n\n# 5. Print Summary Report\nprint(\"\"\"\n=== STEP B.2: ADVANCED DATASET DIMENSION & TEMPORAL INTEGRITY CHECKS ===\n\n--- Dataset Dimensions ---\nProduct Count:   {Product Count}\nCategory Count:  {Category Count}\nDate Range:      {Date Range[0]} to {Date Range[1]}\nStore Count:     {Store Count}\nRegion Count:    {Region Count}\n\n--- Temporal Integrity ---\nMissing dates in sequence: {missing_dates}\n  Details: {missing_dates_details}\nFuture dates detected: {future_dates}\n  Details: {future_dates_details}\nAncient dates detected: {ancient_dates}\n  Details: {ancient_dates_details}\nDates with suspicious record counts: {suspicious_dates}\n  Details: {suspicious_dates_details}\n\"\"\".format(\n    **dimension_metrics,\n    missing_dates=len(missing_dates_list),\n    missing_dates_details=format_dates(missing_dates_list),\n    future_dates=len(future_dates),\n    future_dates_details=format_dates(future_dates),\n    ancient_dates=len(ancient_dates),\n    ancient_dates_details=format_dates(ancient_dates),\n    suspicious_dates=len(suspicious_dates),\n    suspicious_dates_details=suspicious_dates.sort_values(ascending=False).head().to_string() if not suspicious_dates.empty else 'None'\n))\n\n# 6. Store problematic results for downstream review\nproblematic_rows['b2_missing_dates'] = pd.Series(missing_dates_list)\nproblematic_rows['b2_future_dates'] = pd.Series(future_dates)\nproblematic_rows['b2_ancient_dates'] = pd.Series(ancient_dates)\nproblematic_rows['b2_suspicious_dates'] = suspicious_dates\n\n# 7. Actionable Insights\nif len(missing_dates_list) > 0 or len(future_dates) > 0 or len(ancient_dates) > 0 or len(suspicious_dates) > 0:\n    print(\"\\n=== ACTIONABLE INSIGHTS (B.2) ===\\n\")\n    if len(missing_dates_list) > 0:\n        print(\"• Fill or impute missing dates for time series continuity (see 'b2_missing_dates').\")\n        print(problematic_rows['b2_missing_dates'].head())\n    if len(future_dates) > 0 or len(ancient_dates) > 0:\n        print(\"• Correct or flag future/ancient dates (see 'b2_future_dates', 'b2_ancient_dates').\")\n        print(\"Future:\", problematic_rows['b2_future_dates'].head())\n        print(\"Ancient:\", problematic_rows['b2_ancient_dates'].head())\n    if len(suspicious_dates) > 0:\n        print(\"• Investigate dates with suspicious record counts (see 'b2_suspicious_dates').\")\n        print(problematic_rows['b2_suspicious_dates'].head())\nelse:\n    print(\"\\nNo advanced temporal integrity issues found in dataset dimensions.\")","metadata":{"_uuid":"19ead970-c7e7-4ba9-95e7-f3eabe50a020","_cell_guid":"fc9747bb-486b-4027-806c-e92daf6d22f4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:41.535702Z","iopub.execute_input":"2025-07-14T13:40:41.535947Z","iopub.status.idle":"2025-07-14T13:40:41.562266Z","shell.execute_reply.started":"2025-07-14T13:40:41.535926Z","shell.execute_reply":"2025-07-14T13:40:41.561379Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================================\n# Step B.3: Raw Data Import Sanity Check\n# Purpose: To import the dataset in its original, unaltered state to validate the effectiveness\n#          and necessity of the automated cleaning steps.\n# Outputs: A separate \"raw\" DataFrame and a printed report comparing its quality issues against the cleaned version.\n# Rationale: This check provides a definitive baseline of the source data's quality and confirms that\n#            the cleaning process is correctly identifying and resolving issues.\n# ==========================================================================\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Data configuration\nDATA_PATH = Path('/kaggle/input/retail-store-inventory-and-demand-forecasting/sales_data.csv')\nDATE_COL = 'Date'\nDTYPES = {\n    'Store ID': 'category',\n    'Product ID': 'category',\n    'Category': 'category',\n    'Region': 'category',\n    'Inventory Level': 'uint16',\n    'Units Sold': 'uint16',\n    'Units Ordered': 'uint16',\n    'Price': 'float32',\n    'Discount': 'float32',\n    'Weather Condition': 'category',\n    'Promotion': 'bool',\n    'Competitor Pricing': 'float32',\n    'Seasonality': 'category',\n    'Epidemic': 'bool',\n    'Demand': 'uint16'\n}\n\ndef import_raw_data():\n    \"\"\"Import data WITHOUT any corrections to assess original data quality.\"\"\"\n    try:\n        logger.info(\"Importing raw data without corrections...\")\n        print(\"Importing RAW data (no corrections applied)...\")\n        raw_df = pd.read_csv(\n            DATA_PATH,\n            dtype=DTYPES,\n            parse_dates=[DATE_COL],\n            on_bad_lines='warn',\n            encoding='utf-8'\n        )\n        raw_df = raw_df.set_index(DATE_COL).sort_index()\n        logger.info(f\"Raw data imported: {len(raw_df):,} rows\")\n        print(f\"Raw data imported: {len(raw_df):,} rows\")\n        return raw_df\n    except FileNotFoundError:\n        logger.error(f\"Data file not found: {DATA_PATH}\")\n        print(f\"ERROR: Data file not found: {DATA_PATH}\")\n    except pd.errors.DtypeWarning as e:\n        logger.warning(f\"Data type conversion warning: {e}\")\n        print(f\"WARNING: Data type conversion warning: {e}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error importing raw data: {e}\")\n        print(f\"ERROR: Unexpected error importing raw data: {e}\")\n    return None\n\ndef check_raw_data_quality(raw_df):\n    \"\"\"Perform data quality checks on the raw data.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"RAW DATA QUALITY ISSUES (Before Corrections)\")\n    print(\"=\"*60)\n    issues_found = []\n\n    # 1. Negative inventory\n    try:\n        negative_inv = (raw_df['Inventory Level'] < 0).sum()\n        if negative_inv > 0:\n            issues_found.append(f\"Negative Inventory: {negative_inv:,} rows\")\n            print(f\"ISSUE: Negative Inventory: {negative_inv:,} rows\")\n            logger.warning(f\"Negative Inventory: {negative_inv:,} rows\")\n        else:\n            print(\"CLEAN: Negative Inventory: 0 rows\")\n    except Exception as e:\n        logger.error(f\"Error checking negative inventory: {e}\")\n        print(f\"ERROR: Could not check negative inventory: {e}\")\n\n    # 2. Sales exceeding inventory\n    try:\n        sales_exceed = (raw_df['Units Sold'] > raw_df['Inventory Level']).sum()\n        if sales_exceed > 0:\n            issues_found.append(f\"Sales > Inventory: {sales_exceed:,} rows\")\n            print(f\"ISSUE: Sales > Inventory: {sales_exceed:,} rows\")\n            logger.warning(f\"Sales > Inventory: {sales_exceed:,} rows\")\n        else:\n            print(\"CLEAN: Sales > Inventory: 0 rows\")\n    except Exception as e:\n        logger.error(f\"Error checking sales vs inventory: {e}\")\n        print(f\"ERROR: Could not check sales vs inventory: {e}\")\n\n    # 3. Negative prices\n    try:\n        negative_price = (raw_df['Price'] < 0).sum()\n        if negative_price > 0:\n            issues_found.append(f\"Negative Prices: {negative_price:,} rows\")\n            print(f\"ISSUE: Negative Prices: {negative_price:,} rows\")\n            logger.warning(f\"Negative Prices: {negative_price:,} rows\")\n        else:\n            print(\"CLEAN: Negative Prices: 0 rows\")\n    except Exception as e:\n        logger.error(f\"Error checking negative prices: {e}\")\n        print(f\"ERROR: Could not check negative prices: {e}\")\n\n    # 4. Missing values\n    try:\n        missing_total = raw_df.isnull().sum().sum()\n        if missing_total > 0:\n            issues_found.append(f\"Missing Values: {missing_total:,} total\")\n            print(f\"ISSUE: Missing Values: {missing_total:,} total\")\n            logger.warning(f\"Missing Values: {missing_total:,} total\")\n        else:\n            print(\"CLEAN: Missing Values: 0 total\")\n    except Exception as e:\n        logger.error(f\"Error checking missing values: {e}\")\n        print(f\"ERROR: Could not check missing values: {e}\")\n\n    # 5. Invalid dates (index nulls)\n    try:\n        date_issues = raw_df.index.isnull().sum()\n        if date_issues > 0:\n            issues_found.append(f\"Invalid Dates: {date_issues:,} rows\")\n            print(f\"ISSUE: Invalid Dates: {date_issues:,} rows\")\n            logger.warning(f\"Invalid Dates: {date_issues:,} rows\")\n        else:\n            print(\"CLEAN: Invalid Dates: 0 rows\")\n    except Exception as e:\n        logger.error(f\"Error checking date validity: {e}\")\n        print(f\"ERROR: Could not check date validity: {e}\")\n\n    # 6. Check for lists/arrays in Category or other columns\n    # (You can add more columns to this list if needed)\n    cat_cols = ['Category', 'Region', 'Store ID', 'Product ID', 'Weather Condition', 'Seasonality']\n    for col in cat_cols:\n        try:\n            is_list = raw_df[col].apply(lambda x: isinstance(x, (list, tuple, np.ndarray)))\n            if is_list.any():\n                issues_found.append(f\"Lists/arrays in {col}: {is_list.sum():,} rows\")\n                print(f\"ISSUE: Lists/arrays in {col}: {is_list.sum():,} rows\")\n                logger.warning(f\"Lists/arrays in {col}: {is_list.sum():,} rows\")\n                print(\"Sample of problematic rows:\")\n                display(raw_df[is_list].head())\n            else:\n                print(f\"CLEAN: Lists/arrays in {col}: 0 rows\")\n        except Exception as e:\n            logger.error(f\"Error checking lists/arrays in {col}: {e}\")\n            print(f\"ERROR: Could not check lists/arrays in {col}: {e}\")\n\n    return issues_found\n\ndef report_sanity_check_results(issues_found, raw_df, df):\n    print(\"\\n\" + \"=\"*60)\n    print(\"SANITY CHECK RESULTS\")\n    print(\"=\"*60)\n    if issues_found:\n        print(\"VALIDATION: Raw data contains the following quality issues:\")\n        for issue in issues_found:\n            print(f\"  - {issue}\")\n        print(\"\\nThese issues were addressed by the automatic correction process.\")\n        logger.info(f\"Data quality issues detected: {issues_found}\")\n    else:\n        print(\"RESULT: No data quality issues detected in the raw data based on current checks.\")\n        print(\"Interpretation:\")\n        print(\"  - The dataset may be synthetic or pre-cleaned for the challenge.\")\n        print(\"  - Data type coercion or import settings are unlikely to mask issues, but can be reviewed if needed.\")\n        print(\"  - If real-world data is expected, further investigation is recommended.\")\n        logger.info(\"No data quality issues found in raw data.\")\n\n    print(\"\\nComparison with corrected data:\")\n    print(f\"Raw data rows: {len(raw_df):,}\")\n    print(f\"Corrected data rows: {len(df):,}\")\n    print(f\"Row difference: {len(df) - len(raw_df):,}\")\n\n    # Show sample of raw data\n    print(\"\\nRaw data sample:\")\n    display(raw_df.head())\n\n    # Key statistics comparison\n    print(\"\\nKey statistics comparison (Raw vs Corrected):\")\n    comparison_cols = ['Inventory Level', 'Units Sold', 'Price']\n    for col in comparison_cols:\n        try:\n            print(f\"\\n{col}:\")\n            print(f\"  Raw       - Min: {raw_df[col].min():.2f}, Max: {raw_df[col].max():.2f}\")\n            print(f\"  Corrected - Min: {df[col].min():.2f}, Max: {df[col].max():.2f}\")\n        except Exception as e:\n            logger.error(f\"Error comparing statistics for {col}: {e}\")\n            print(f\"ERROR: Could not compare statistics for {col}: {e}\")\n\n# Main execution\nraw_df = import_raw_data()\nif raw_df is not None:\n    issues_found = check_raw_data_quality(raw_df)\n    # 'df' should be your corrected/processed DataFrame, assumed to be loaded elsewhere\n    try:\n        report_sanity_check_results(issues_found, raw_df, df)\n    except Exception as e:\n        logger.error(f\"Error during reporting: {e}\")\n        print(f\"ERROR: Could not complete reporting: {e}\")\nelse:\n    logger.error(\"Failed to import raw data for comparison\")\n    print(\"FAILED: Could not import raw data for comparison\")","metadata":{"_uuid":"2bc6940c-74a5-4626-a351-bfa098ceddb5","_cell_guid":"0fe14ee9-f4f5-4769-a831-717c1404019d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:41.563373Z","iopub.execute_input":"2025-07-14T13:40:41.563695Z","iopub.status.idle":"2025-07-14T13:40:42.093043Z","shell.execute_reply.started":"2025-07-14T13:40:41.563664Z","shell.execute_reply":"2025-07-14T13:40:42.092298Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Advanced Data Consistency & Plausibility Checks\n\n**Purpose**\n\nWhile the raw data passes all standard quality checks (negatives, missing values, logical errors, etc.), subtle issues may still exist, especially in synthetic datasets. To ensure robust modeling and reliable insights, we perform additional advanced checks. The previous approach focused on price and category consistency across regions. Given the observed wild price variation and to deepen our understanding, we now also examine:\n\n- **Pricing consistency within each region by store and item**\n- **Category consistency within each region by item**\n- **Price and category consistency across all regions**\n\nThese checks help uncover hidden artifacts, mislabeling, or implausible economic patterns that could impact model performance and interpretation.\n\n---\n\n**Stages of Advanced Checks**\n\n**Stage 1: Pricing Consistency Within Each Region by Store and Item**  \n*Check if the same product shows plausible price variation within each region across different stores. Detect unrealistic uniformity or outliers, and compare pricing patterns with competitors within the region. This exposes extreme values, pricing errors, and ensures that price deviations are justifiable (e.g., due to promotions or local market conditions).*\n\n**Stage 2: Category Consistency Within Each Region**  \n*Ensure each product is consistently assigned to the same category in all records within each region. Flag any mislabeled products or inconsistent categorization within a region.*\n\n**Stage 3: Price vs. Competitor Pricing Analysis Within Regions**  \n*Analyze the relationship between store prices and competitor pricing within each region. Identify implausible gaps, artificial patterns, or deviations from expected pricing strategies.*\n\n**Stage 4: Item Category Mapping Consistency Across All Regions**  \n*Ensure each product is mapped to the same category across all regions. Flag any products with inconsistent category assignments, which could indicate mislabeling or data entry errors.*\n\n---","metadata":{"_uuid":"a78d028a-126b-4ab5-a9b1-0ee040449c0e","_cell_guid":"d9331c89-24bb-4888-ae5c-b25ce20b55a7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step C.1: Intra-Regional Price Consistency Check\n# Purpose: To ensure that price variation for each product across different stores within the same region is plausible.\n# Outputs: An audit log, a dictionary of problematic row indices, and visualizations of price distributions.\n# Rationale: This check helps detect data artifacts, pricing errors, or unrealistic uniformity by verifying that\n#            price deviations are statistically justifiable (e.g., due to promotions or local market conditions).\n# ==========================================================================\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\n\n# Configure logging for traceability\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# ==========================================================================\n# Helper: IQR-based outlier detection\n# ==========================================================================\ndef iqr_outliers(data, k=1.5):\n    \"\"\"Detect outliers using the IQR method. Returns a boolean mask of outliers.\"\"\"\n    q1 = np.percentile(data, 25)\n    q3 = np.percentile(data, 75)\n    iqr = q3 - q1\n    lower_bound = q1 - k * iqr\n    upper_bound = q3 + k * iqr\n    return (data < lower_bound) | (data > upper_bound)\n\n# ==========================================================================\n# Main check: Intra-regional price consistency\n# ==========================================================================\ndef check_pricing_within_region(\n    df,\n    region_col='Region',\n    store_col='Store ID',\n    product_col='Product ID',\n    price_col='Price',\n    competitor_col='Competitor Pricing'\n):\n    \"\"\"\n    Check for plausible price variation within each region by store and item.\n    Detects unrealistic uniformity, outliers, and compares with competitor prices.\n    Logs errors, stores problematic row indices, and provides statistics and visualizations.\n    Returns pd.Index of problematic rows.\n    \"\"\"\n    log = []\n    log.append(\"=== INTRA-REGIONAL PRICE CONSISTENCY AUDIT LOG ===\\n\\n\")\n    problematic_row_indices = pd.Index([])\n\n    try:\n        # ==========================================================================\n        # Check for required columns\n        # ==========================================================================\n        required_cols = [region_col, store_col, product_col, price_col]\n        if competitor_col in df.columns:\n            required_cols.append(competitor_col)\n        missing_cols = [col for col in required_cols if col not in df.columns]\n        if missing_cols:\n            log.append(f\"Missing required columns: {missing_cols}\\n\")\n            logger.error(f\"Missing required columns: {missing_cols}\")\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        log.append(\"All required columns present.\\n\")\n\n        # ==========================================================================\n        # Group data by region, product, and store for price analysis\n        # ==========================================================================\n        log.append(\"Grouping data by region, product, and store...\\n\")\n        grouped = df.groupby([region_col, product_col, store_col], observed=True)[price_col].mean().reset_index()\n\n        # ==========================================================================\n        # Analyze price variation and log issues\n        # ==========================================================================\n        uniformity_issues = []\n        outlier_issues = []\n        for (region, product), group in grouped.groupby([region_col, product_col], observed=True):\n            prices = group[price_col]\n            n_stores = len(prices)\n            if n_stores > 1:\n                # Outlier detection\n                outliers = iqr_outliers(prices.values)\n                if outliers.any():\n                    outlier_stores = group[store_col][outliers].tolist()\n                    outlier_prices = prices[outliers].tolist()\n                    outlier_issues.append((region, product, outlier_stores, outlier_prices))\n                    for store in outlier_stores:\n                        mask = (df[region_col] == region) & (df[product_col] == product) & (df[store_col] == store)\n                        problematic_row_indices = problematic_row_indices.union(df[mask].index)\n                # Uniformity detection\n                if prices.nunique() == 1:\n                    uniformity_issues.append((region, product, prices.iloc[0]))\n\n        # ==========================================================================\n        # Statistical summary\n        # ==========================================================================\n        log.append(\"\\n=== STATISTICAL SUMMARY ===\\n\")\n        stats = grouped.groupby([region_col, product_col], observed=True)[price_col].agg(['count', 'mean', 'std', 'min', 'max'])\n        log.append(stats.to_string() + \"\\n\")\n\n        # ==========================================================================\n        # Uniformity and outlier issues summary\n        # ==========================================================================\n        log.append(\"\\n=== UNIFORMITY ISSUES ===\\n\")\n        if uniformity_issues:\n            uniform_df = pd.DataFrame(uniformity_issues, columns=['Region', 'Product', 'Price'])\n            log.append(f\"Products with identical prices in all stores within a region: {len(uniformity_issues)}\\n\")\n            log.append(uniform_df.groupby('Region').size().rename('Uniform Products Count').to_string() + \"\\n\")\n            log.append(\"Sample:\\n\" + uniform_df.head(10).to_string() + \"\\n\")\n        else:\n            log.append(\"No products with identical prices in all stores within a region.\\n\")\n\n        log.append(\"\\n=== OUTLIER ISSUES ===\\n\")\n        if outlier_issues:\n            log.append(f\"Products with outlier prices in some stores within a region: {len(outlier_issues)}\\n\")\n            for i, (region, product, stores, prices) in enumerate(outlier_issues[:10]):\n                log.append(f\"Region {region}, Product {product}, Stores {stores}, Prices {prices}\\n\")\n        else:\n            log.append(\"No products with outlier prices in some stores within a region.\\n\")\n\n        # ==========================================================================\n        # Problematic row indices summary\n        # ==========================================================================\n        log.append(\"\\n=== PROBLEMATIC ROWS ===\\n\")\n        if len(problematic_row_indices) > 0:\n            log.append(f\"Total problematic rows: {len(problematic_row_indices)}\\n\")\n            log.append(f\"Problematic rows index (Date):\\n{problematic_row_indices}\\n\")\n        else:\n            log.append(\"No problematic rows detected.\\n\")\n\n        # ==========================================================================\n        # Visual analysis with seaborn\n        # ==========================================================================\n        try:\n            if len(grouped) > 0:\n                # Boxplot: Price distribution by product in a sample region\n                sample_region = grouped[region_col].iloc[0]\n                sample_df = grouped[grouped[region_col] == sample_region]\n                plt.figure(figsize=(12, 6))\n                sns.boxplot(data=sample_df, x=product_col, y=price_col)\n                plt.title(f'Price Distribution by Product in Region {sample_region}')\n                plt.xticks(rotation=45)\n                plt.tight_layout()\n                plt.show()\n\n                # Scatterplot: Price vs Competitor Price (if available)\n                if competitor_col in df.columns:\n                    sample_df = df[df[region_col] == sample_region]\n                    plt.figure(figsize=(12, 6))\n                    sns.scatterplot(data=sample_df, x=price_col, y=competitor_col, hue=product_col)\n                    plt.title(f'Price vs Competitor Price in Region {sample_region}')\n                    plt.tight_layout()\n                    plt.show()\n        except Exception as e:\n            log.append(f\"Error during visualization: {e}\\n\")\n\n    except Exception as e:\n        log.append(f\"Critical error during intra-regional price consistency check: {e}\\n\")\n        logger.error(f\"Critical error during intra-regional price consistency check: {e}\")\n\n    # ==========================================================================\n    # Print audit log to cell output\n    # ==========================================================================\n    print(''.join(log))\n    print(\"\\nIntra-regional price consistency checks completed. Problematic row indices are stored in 'problematic_row_indices'.\")\n\n    return problematic_row_indices\n\n# ==========================================================================\n# Run the check and store problematic row indices for review\n# ==========================================================================\nproblematic_row_indices = check_pricing_within_region(\n    df,\n    region_col='Region',\n    store_col='Store ID',\n    product_col='Product ID',\n    price_col='Price',\n    competitor_col='Competitor Pricing'\n)\n\n# ==========================================================================\n# Optional: Print problematic rows for review\n# ==========================================================================\nif len(problematic_row_indices) > 0:\n    print(\"\\n=== PROBLEMATIC ROWS ===\\n\")\n    print(df.loc[problematic_row_indices])\nelse:\n    print(\"\\nNo problematic rows found.\")","metadata":{"_uuid":"6a4d4193-d2c7-40e3-8744-18057811636e","_cell_guid":"ec39c1f0-5fd0-4ea6-b43f-0f3e3dc1ac55","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:42.094005Z","iopub.execute_input":"2025-07-14T13:40:42.094278Z","iopub.status.idle":"2025-07-14T13:40:44.041658Z","shell.execute_reply.started":"2025-07-14T13:40:42.094249Z","shell.execute_reply":"2025-07-14T13:40:44.040666Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================================\n# Step C.2: Intra-Regional Category Consistency Check\n# Purpose: To ensure each product is consistently assigned to the same category in all records within a given region.\n# Outputs: An audit log and a dictionary of row indices for any products with inconsistent category assignments.\n# Rationale: This check prevents ambiguity and misrepresentation of products at the regional level, ensuring that\n#            regional analysis and modeling are based on accurate category data.\n# ==========================================================================\n\ndef check_category_consistency(df, region_col='Region', product_col='Product ID', category_col='Category'):\n    \"\"\"\n    Check for consistent category assignment within each region and product.\n    Log inconsistencies and return problematic row indices.\n    \"\"\"\n    log = []\n    inconsistent_indices = pd.Index([])\n    log.append(\"=== CATEGORY CONSISTENCY WITHIN REGION AUDIT LOG ===\\n\\n\")\n\n    try:\n        # Group by region and product, count unique categories\n        group = df.groupby([region_col, product_col])[category_col].nunique().reset_index()\n        inconsistent = group[group[category_col] > 1]\n        \n        if len(inconsistent) > 0:\n            log.append(f\"Found {len(inconsistent)} product-region pairs with inconsistent category assignment.\\n\")\n            log.append(\"Sample of inconsistent pairs:\\n\")\n            log.append(inconsistent.head(10).to_string() + \"\\n\")\n            \n            # Store indices of problematic rows\n            for _, row in inconsistent.iterrows():\n                mask = (df[region_col] == row[region_col]) & (df[product_col] == row[product_col])\n                inconsistent_indices = inconsistent_indices.union(df[mask].index)\n        else:\n            log.append(\"No inconsistent category assignments found within any region.\\n\")\n    except Exception as e:\n        log.append(f\"Error during category consistency check: {e}\\n\")\n    \n    print(''.join(log))\n    return inconsistent_indices\n\n# Run the check\ninconsistent_category_indices = check_category_consistency(\n    df,\n    region_col='Region',\n    product_col='Product ID',\n    category_col='Category'\n)\n\n# Optional: Print problematic rows for review\nif len(inconsistent_category_indices) > 0:\n    print(\"\\n=== PROBLEMATIC ROWS ===\\n\")\n    print(df.loc[inconsistent_category_indices])\nelse:\n    print(\"\\nNo problematic rows found.\")","metadata":{"_uuid":"2eddf77e-96e3-459d-b1f9-c9b4c03d2a11","_cell_guid":"656bb26d-e27c-4d89-bb13-e5152d4bffc6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:44.042663Z","iopub.execute_input":"2025-07-14T13:40:44.042968Z","iopub.status.idle":"2025-07-14T13:40:44.245775Z","shell.execute_reply.started":"2025-07-14T13:40:44.042946Z","shell.execute_reply":"2025-07-14T13:40:44.244865Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next Steps: Correcting Category Mislabeling\n\n## **Findings Summary**\n\n- **Inconsistent Category Assignments:**  \n  - 16 product-region pairs were identified with inconsistent category assignments.\n  - Example: Multiple categories (e.g., \"Electronics\" and \"Clothing\") assigned to the same product within a region.\n- **Problematic Rows:**  \n  - Specific rows with inconsistent labels have been flagged and can be reviewed or corrected as needed.\n\n## **Action Plan**\n\n1. **Majority Vote Correction:**  \n   - For each product-region pair, determine the most frequently assigned category (the mode).\n   - Update all records for that product-region pair to use the majority category.\n\n\n\n2. **Manual Review for Edge Cases:**  \n   - For cases where the majority is unclear or validation is inconclusive, flag for manual review.\n\n3. **Update the Dataset:**  \n   - Replace inconsistent category labels with the corrected value.\n   - Ensure all changes are documented for traceability.\n\n4. **Re-run Consistency Checks:**  \n   - Rerun the category consistency check to confirm that all issues have been resolved.\n   - Repeat the process if any inconsistencies remain.","metadata":{"_uuid":"bd56249b-3458-45ae-8d7d-c65050b10eba","_cell_guid":"58574e2b-ec01-4e07-bc7e-38b922464f3f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step C.2.1: Correct Intra-Regional Category Assignments\n# Purpose: To automatically correct inconsistent product category assignments within each region by applying a majority vote.\n# Outputs: An updated DataFrame with corrected category labels and a confirmation that no inconsistencies remain.\n# Rationale: This automated correction ensures that each product has a single, unambiguous category within its region,\n#            resolving data conflicts before proceeding to cross-regional analysis.\n# ==========================================================================\n\n\n\n# ==========================================================================\n# 1. Majority Vote Correction\n# ==========================================================================\n# Group by region and product, find the most common category for each group\nmajority_category = (\n    df.groupby(['Region', 'Product ID'])['Category']\n    .apply(lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0])\n    .reset_index()\n    .rename(columns={'Category': 'Corrected_Category'})\n)\n\n# Merge the corrected categories back into the original DataFrame\ndf = df.merge(\n    majority_category,\n    on=['Region', 'Product ID'],\n    how='left'\n)\n\n# Replace the original category column with the corrected one\ndf['Category'] = df['Corrected_Category']\ndf = df.drop(columns=['Corrected_Category'])\n\n\n# ==========================================================================\n# 2. Manual Review for Edge Cases (Optional)\n# ==========================================================================\n# For cases where the majority is unclear (e.g., ties or ambiguous features),\n# you can flag these for manual review. Example:\n# Find groups with multiple mode values (ties)\ntie_groups = (\n    df.groupby(['Region', 'Product ID'])['Category']\n    .apply(lambda x: len(x.mode()) > 1)\n    .reset_index(name='Is_Tie')\n)\ntie_groups = tie_groups[tie_groups['Is_Tie']]\nif not tie_groups.empty:\n    print(\"The following product-region pairs have category ties (manual review recommended):\")\n    print(tie_groups)\nelse:\n    print(\"No category ties found. All assignments are unambiguous.\")\n\n# ==========================================================================\n# 3. Update the Dataset\n# ==========================================================================\n# The DataFrame is already updated above. This step is complete.\n\n# ==========================================================================\n# 4. Re-run Consistency Checks\n# ==========================================================================\n# Re-run your category consistency check to confirm all issues are resolved.\n# (Use the same function as before.)\ninconsistent_category_indices = check_category_consistency(\n    df,\n    region_col='Region',\n    product_col='Product ID',\n    category_col='Category'\n)\n\n# Optionally, print problematic rows if any are found\nif len(inconsistent_category_indices) > 0:\n    print(\"\\n=== PROBLEMATIC ROWS AFTER CORRECTION ===\\n\")\n    print(df.loc[inconsistent_category_indices])\nelse:\n    print(\"\\nNo problematic rows found after correction. Category assignments are now consistent.\")","metadata":{"_uuid":"df7aab7b-e676-4869-9713-1929439888bf","_cell_guid":"c1e8bde7-a027-438e-8b54-b3a848fca865","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:44.246785Z","iopub.execute_input":"2025-07-14T13:40:44.247210Z","iopub.status.idle":"2025-07-14T13:40:44.333070Z","shell.execute_reply.started":"2025-07-14T13:40:44.247179Z","shell.execute_reply":"2025-07-14T13:40:44.332233Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================================\n# Step C.3: Price vs. Competitor Pricing Analysis\n# Purpose: To analyze the relationship between store prices and competitor pricing within each region.\n# Outputs: A summary of findings, visualizations, and a list of row indices with implausible price gaps or ratios.\n# Rationale: This check helps identify artificial patterns, data entry errors, or deviations from expected\n#            pricing strategies that could impact model interpretation and accuracy.\n# ==========================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Check if 'Competitor Pricing' column exists\nif 'Competitor Pricing' not in df.columns:\n    print(\"No 'Competitor Pricing' column found. Skipping Step 3.\")\nelse:\n    # ==========================================================================\n    # 1. Price vs. Competitor Price by Region\n    # ==========================================================================\n    # Calculate price differences and ratios\n    df['Price_Diff'] = df['Price'] - df['Competitor Pricing']\n    df['Price_Ratio'] = df['Price'] / df['Competitor Pricing']\n\n    # ==========================================================================\n    # 2. Identify Implausible Gaps or Deviations\n    # ==========================================================================\n    # Define thresholds for implausible gaps (adjust as needed)\n    price_diff_threshold = 50  # Example: absolute difference threshold\n    price_ratio_threshold = 0.5  # Example: ratio threshold (Price is less than half or more than double)\n    price_ratio_upper = 2.0     # Example: upper ratio threshold\n\n    # Flag rows with implausible gaps or ratios\n    df['Implausible_Gap'] = df['Price_Diff'].abs() > price_diff_threshold\n    df['Implausible_Ratio'] = (df['Price_Ratio'] < price_ratio_threshold) | (df['Price_Ratio'] > price_ratio_upper)\n\n    # Combine flags for any implausible condition\n    df['Implausible_Pricing'] = df['Implausible_Gap'] | df['Implausible_Ratio']\n\n    # ==========================================================================\n    # 3. Summarize and Log Findings\n    # ==========================================================================\n    print(\"=== PRICE VS. COMPETITOR PRICING ANALYSIS ===\")\n    print(f\"Found {df['Implausible_Pricing'].sum()} rows with implausible price gaps or ratios.\")\n    if df['Implausible_Pricing'].sum() > 0:\n        print(\"\\nSample of rows with implausible pricing:\")\n        print(df[df['Implausible_Pricing']].head(10))\n    else:\n        print(\"\\nNo implausible price gaps or ratios detected.\")\n\n    # ==========================================================================\n    # 4. Visualize Price vs. Competitor Pricing by Region\n    # ==========================================================================\n    plt.figure(figsize=(12, 6))\n    sns.scatterplot(data=df, x='Price', y='Competitor Pricing', hue='Region', alpha=0.7)\n    plt.title('Price vs. Competitor Pricing by Region')\n    plt.axline((0, 0), slope=1, color='red', linestyle='--')\n    plt.tight_layout()\n    plt.show()\n\n    # ==========================================================================\n    # 5. Store Indices of Implausible Pricing for Review\n    # ==========================================================================\n    # Store indices for follow-up (optional)\n    implausible_indices = df[df['Implausible_Pricing']].index\n    print(\"\\nIndices of rows with implausible pricing (for review):\")\n    print(implausible_indices)","metadata":{"_uuid":"f9e35d55-4366-442e-8ff8-f12968feb5a7","_cell_guid":"21a39463-0612-4661-8ac4-4c3d2f50e38e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:44.335598Z","iopub.execute_input":"2025-07-14T13:40:44.335880Z","iopub.status.idle":"2025-07-14T13:40:46.965706Z","shell.execute_reply.started":"2025-07-14T13:40:44.335856Z","shell.execute_reply":"2025-07-14T13:40:46.964735Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================================\n# Step C.4: Global Category Consistency Check\n# Purpose: To ensure each product is mapped to the same category across all regions, establishing a global standard.\n# Outputs: An audit log and a list of product IDs and row indices that have inconsistent category assignments.\n# Rationale: This global check is crucial for maintaining data integrity across the entire dataset, ensuring that a\n#            product is treated as the same entity regardless of its region, which is vital for accurate modeling.\n# ==========================================================================\n\nimport pandas as pd\n\ndef check_global_category_consistency(df, product_col='Product ID', category_col='Category'):\n    \"\"\"\n    Check if each product is assigned to the same category across all regions.\n    Log inconsistencies and return problematic product IDs.\n    \"\"\"\n    log = []\n    log.append(\"=== GLOBAL CATEGORY MAPPING CONSISTENCY AUDIT LOG ===\\n\\n\")\n\n    # Group by product and count unique categories\n    group = df.groupby(product_col)[category_col].nunique().reset_index()\n    inconsistent_products = group[group[category_col] > 1]\n\n    if len(inconsistent_products) > 0:\n        log.append(f\"Found {len(inconsistent_products)} products with inconsistent category assignments across regions.\\n\")\n        log.append(\"Sample of inconsistent products:\\n\")\n        log.append(inconsistent_products.head(10).to_string() + \"\\n\")\n\n        # Store indices of problematic rows for review\n        problematic_products = inconsistent_products[product_col].tolist()\n        problematic_indices = df[df[product_col].isin(problematic_products)].index\n        log.append(f\"Indices of rows with inconsistent global category assignments:\\n{problematic_indices}\\n\")\n    else:\n        log.append(\"No products with inconsistent category assignments across regions.\\n\")\n        problematic_indices = pd.Index([])\n\n    print(''.join(log))\n    return problematic_indices\n\n# Run the check\nglobal_inconsistent_indices = check_global_category_consistency(\n    df,\n    product_col='Product ID',\n    category_col='Category'\n)\n\n# Optional: Print problematic rows for review\nif len(global_inconsistent_indices) > 0:\n    print(\"\\n=== PROBLEMATIC ROWS FOR GLOBAL CATEGORY CONSISTENCY ===\\n\")\n    print(df.loc[global_inconsistent_indices])\nelse:\n    print(\"\\nNo problematic rows found for global category consistency.\")","metadata":{"_uuid":"b39dc480-2ed2-46f8-969c-f41ae842effe","_cell_guid":"189eb9ff-fcac-4bd5-adcb-c7ff45096f74","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:46.966540Z","iopub.execute_input":"2025-07-14T13:40:46.966768Z","iopub.status.idle":"2025-07-14T13:40:47.012481Z","shell.execute_reply.started":"2025-07-14T13:40:46.966747Z","shell.execute_reply":"2025-07-14T13:40:47.011395Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Summary: Global Category Mapping Consistency\n\n## **Findings**\n\n- **Inconsistent Category Assignments:**  \n  - **19 products** were found to have inconsistent category assignments across regions.\n  - **Example:** Product P0001 is assigned to 3 different categories across regions; P0003 to 4 different categories.\n  - **Impact:** This inconsistency can lead to errors in reporting, analysis, and modeling, as the same product is treated differently in different regions.\n\n- **Root Cause Analysis:**  \n  - **Intra-region consistency** was achieved in Stage 2 (each product has only one category per region).\n  - **Inter-region inconsistency** remains (different regions assign different categories to the same product).\n  - **Previous corrections did not cause this issue**—they simply revealed a broader data quality challenge.\n\n## **Corrective Steps**\n\n1. **Analyze Category Assignments by Region:**  \n   - For each inconsistent product, review the categories assigned in each region.\n   - Identify patterns or root causes (e.g., data entry errors, naming conventions, or genuine business rules).\n\n2. **Standardize Category Mappings:**  \n   - **Establish a global standard** for each product’s category.\n   - **Update all records** to use the agreed-upon category, regardless of region.\n\n3. **Document Region-Specific Exceptions:**  \n   - If certain products must have different categories in specific regions for valid business reasons, document these as exceptions.\n   - Ensure these exceptions are clearly flagged and justified.\n\n4. **Re-run Consistency Checks:**  \n   - After making corrections, re-run the global category consistency check to confirm all issues are resolved.\n   - Repeat the process if any inconsistencies remain.\n\n5. **Maintain Ongoing Audits:**  \n   - **Regularly audit** category assignments as new products or regions are added.\n   - **Update your category taxonomy** as needed to reflect business changes.\n\n## **Why This Matters**\n\n- **Data Integrity:** Ensures that products are treated consistently across all regions, reducing errors in analysis and reporting.\n- **Actionable Insights:** Supports accurate demand forecasting, assortment planning, and marketing strategies.\n- **Scalability:** Makes it easier to integrate new products and regions into your data pipeline.","metadata":{"_uuid":"f828a7f4-28c2-48a6-bb34-27da55088220","_cell_guid":"a4d411c1-462d-4b72-8ec2-1c75315573e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step C.4.1: Correct Global Category Assignments\n# Purpose: To enforce a single, global category for each product across all regions by assigning the most common (mode) category.\n# Outputs: A corrected DataFrame and a final confirmation that all cross-regional category inconsistencies are resolved.\n# Rationale: This final correction step establishes a definitive \"source of truth\" for product categorization, ensuring\n#            the highest level of data consistency before feature engineering and modeling.\n# ==========================================================================\n\nimport pandas as pd\n\ndef check_global_category_consistency(df, product_col='Product ID', category_col='Category'):\n    \"\"\"\n    Check if each product is assigned to the same category across all regions.\n    Log inconsistencies and return problematic product IDs and indices.\n    \"\"\"\n    # Group by product and count unique categories\n    group = df.groupby(product_col)[category_col].nunique().reset_index()\n    inconsistent_products = group[group[category_col] > 1]\n\n    if len(inconsistent_products) > 0:\n        print(f\"Found {len(inconsistent_products)} products with inconsistent category assignments across regions.\")\n        print(\"Sample of inconsistent products:\")\n        print(inconsistent_products.head(10))\n        # Store indices of problematic rows for review\n        problematic_products = inconsistent_products[product_col].tolist()\n        problematic_indices = df[df[product_col].isin(problematic_products)].index\n        print(f\"\\nIndices of rows with inconsistent global category assignments:\\n{problematic_indices}\")\n    else:\n        print(\"No products with inconsistent category assignments across regions.\")\n        problematic_indices = pd.Index([])\n    return problematic_indices\n\ndef fix_global_category_consistency(df, product_col='Product ID', category_col='Category'):\n    \"\"\"\n    Ensure each product is mapped to the same category across all regions.\n    Assign the most common (mode) category for each product as the global standard.\n    \"\"\"\n    # Find the most common (mode) category for each product across all regions\n    # If there are ties, take the first mode\n    mode_category = (\n        df.groupby(product_col)[category_col]\n        .apply(lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0])\n        .reset_index()\n        .rename(columns={category_col: 'Global_Category'})\n    )\n    \n    # Merge the global category back into the original DataFrame\n    df = df.merge(\n        mode_category,\n        on=product_col,\n        how='left'\n    )\n    \n    # Replace the original category column with the global category\n    df[category_col] = df['Global_Category']\n    df = df.drop(columns=['Global_Category'])\n    return df\n\n# Apply the correction\ndf = fix_global_category_consistency(df, product_col='Product ID', category_col='Category')\n\n# Re-run the global category consistency check to confirm all issues are resolved\nglobal_inconsistent_indices = check_global_category_consistency(\n    df,\n    product_col='Product ID',\n    category_col='Category'\n)\n\nif len(global_inconsistent_indices) > 0:\n    print(\"\\n=== WARNING: PROBLEMATIC ROWS FOR GLOBAL CATEGORY CONSISTENCY ===\\n\")\n    print(df.loc[global_inconsistent_indices])\nelse:\n    print(\"\\nSUCCESS: No problematic rows found for global category consistency after correction.\")","metadata":{"_uuid":"620f547f-b132-46c3-ac69-630a57b0e9cc","_cell_guid":"d177af6c-41e0-4679-969f-e9a3beaaf316","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:47.013719Z","iopub.execute_input":"2025-07-14T13:40:47.014082Z","iopub.status.idle":"2025-07-14T13:40:47.062484Z","shell.execute_reply.started":"2025-07-14T13:40:47.014049Z","shell.execute_reply":"2025-07-14T13:40:47.061634Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================================\n# Step D: Demand Characteristics Analysis\n# Purpose: To analyze demand patterns to inform model selection and feature engineering.\n# Outputs: Summary statistics and visualizations of demand distribution, zero-demand frequency, and the impact of promotions/events.\n# Rationale: Understanding the fundamental characteristics of the demand signal is essential for choosing an appropriate\n#            forecasting model and engineering features that capture relevant business dynamics.\n# ==========================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- 1. Value Range (Min/Max) ---\nprint(\"=\"*50)\nprint(\"Demand (Units Sold): Summary Statistics\")\nprint(\"=\"*50)\nprint(df['Units Sold'].describe())\nprint(\"\\n\")\n\n# --- 2. Zero-Demand Frequency ---\nzero_demand = (df['Units Sold'] == 0).sum()\ntotal_rows = len(df)\nprint(f\"Zero-demand rows: {zero_demand} ({zero_demand/total_rows:.2%})\")\nprint(\"\\n\")\n\n# --- 3. Epidemic & Promotion Variance ---\nprint(\"=\"*50)\nprint(\"Epidemic and Promotion Impact on Demand\")\nprint(\"=\"*50)\nprint(\"Epidemic periods:\", df['Epidemic'].sum(), \"rows\")\nprint(\"Promotion periods:\", df['Promotion'].sum(), \"rows\")\nprint(\"\\n\")\n\nif df['Epidemic'].sum() > 0:\n    print(\"Average Units Sold during Epidemic:\", df[df['Epidemic']]['Units Sold'].mean())\n    print(\"Average Units Sold outside Epidemic:\", df[~df['Epidemic']]['Units Sold'].mean())\n    print(\"\\n\")\nif df['Promotion'].sum() > 0:\n    print(\"Average Units Sold during Promotion:\", df[df['Promotion']]['Units Sold'].mean())\n    print(\"Average Units Sold outside Promotion:\", df[~df['Promotion']]['Units Sold'].mean())\n    print(\"\\n\")\n\n# --- 4. Regional Patterns ---\nprint(\"=\"*50)\nprint(\"Regional Demand Patterns\")\nprint(\"=\"*50)\nprint(df.groupby('Region')['Units Sold'].describe())\nprint(\"\\n\")\n\n# --- 5. Intuitive Visualizations ---\n\n# A. Histogram with KDE\nplt.figure(figsize=(12, 6))\nsns.histplot(df['Units Sold'], kde=True, bins=50, color='skyblue', alpha=0.8)\n# Add mean and median lines\nmean_units_sold = df['Units Sold'].mean()\nmedian_units_sold = df['Units Sold'].median()\nplt.axvline(mean_units_sold, color='red', linestyle='--', label=f'Mean: {mean_units_sold:.2f}')\nplt.axvline(median_units_sold, color='green', linestyle='-', label=f'Median: {median_units_sold:.2f}')\nplt.title('Distribution of Units Sold (All Regions)', fontsize=16)\nplt.xlabel('Units Sold', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n# B. Faceted Histogram by Region\nplt.figure(figsize=(14, 6))\ng = sns.FacetGrid(df, col='Region', col_wrap=2, height=4)\ng.map(sns.histplot, 'Units Sold', kde=True, bins=30, color='lightcoral', alpha=0.8)\ng.set_axis_labels(\"Units Sold\", \"Frequency\")\ng.set_titles(\"Region: {col_name}\")\ng.fig.suptitle('Distribution of Units Sold by Region', y=1.05, fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent title overlap\nplt.show()\n\n# C. Violin Plot by Region\nplt.figure(figsize=(12, 6))\nsns.violinplot(data=df, x='Region', y='Units Sold', palette='viridis')\nplt.title('Units Sold Distribution by Region (Violin Plot)', fontsize=16)\nplt.xlabel('Region', fontsize=12)\nplt.ylabel('Units Sold', fontsize=12)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n# D. Scatterplot of Demand vs. Forecast (if available)\nif 'Demand' in df.columns:\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='Demand', y='Units Sold', alpha=0.4, color='darkblue')\n    plt.title('Forecast Demand vs. Actual Units Sold', fontsize=16)\n    plt.xlabel('Forecast Demand', fontsize=12)\n    plt.ylabel('Actual Units Sold', fontsize=12)\n    plt.plot([df['Demand'].min(), df['Demand'].max()], [df['Demand'].min(), df['Demand'].max()], 'r--', label='Perfect Forecast')\n    \n    # Add correlation coefficient as annotation\n    correlation = df['Demand'].corr(df['Units Sold'])\n    plt.annotate(f'Correlation (R): {correlation:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12, color='red')\n\n    plt.legend()\n    plt.grid(axis='both', linestyle='--', alpha=0.7)\n    plt.show()\n\n# --- 6. Additional Insights (Optional) ---\nif 'Demand' in df.columns:\n    print(\"=\"*50)\n    print(\"Forecast Demand vs. Actual Units Sold\")\n    print(\"=\"*50)\n    print(df[['Demand', 'Units Sold']].describe())\n    print(\"\\nCorrelation between Demand and Units Sold:\", df['Demand'].corr(df['Units Sold']))\n    print(\"\\n\")\n\nprint(\"=\"*50)\nprint(\"Part C: Demand Characteristics Analysis Complete\")\nprint(\"=\"*50)","metadata":{"_uuid":"32c7dd5e-f1c8-46db-9f4f-baae5a490d55","_cell_guid":"ee3d28b0-e3a6-4159-9a54-479cfb955d7a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:47.063613Z","iopub.execute_input":"2025-07-14T13:40:47.063952Z","iopub.status.idle":"2025-07-14T13:40:51.242456Z","shell.execute_reply.started":"2025-07-14T13:40:47.063920Z","shell.execute_reply":"2025-07-14T13:40:51.241512Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part C: Demand Characteristics – Summary\n\n**Findings Align with Expectations:**  \nThe analysis of demand characteristics reveals patterns that are consistent with both business logic and real-world events—notably, the significant impact of epidemic (COVID-19) periods on demand. As expected, units sold are lower during these times, reflecting the disruption caused by the pandemic. Promotions, on the other hand, are shown to effectively increase sales. Demand is relatively stable across regions, with minor variations and some outliers, but no systemic issues are detected.\n\n**Implications for Part 3 (Insights-to-Action Mapping):**  \n- **Epidemic Demand Spikes:**  \n  - **Action:** Implement shock indicator features in forecasting models to account for COVID-19 disruptions.\n- **Promotion Variance:**  \n  - **Action:** Continue to monitor and optimize promotion strategies, and include promotion flags as features in models.\n- **Regional Patterns:**  \n  - **Action:** Maintain regional segmentation in analysis and consider regional dummy variables in models to capture local effects.\n- **Zero-Demand Rows:**  \n  - **Action:** Validate a sample of zero-demand records to ensure data accuracy, but expect a small number due to intermittent demand.\n- **Forecast Accuracy:**  \n  - **Action:** Review forecasting methodology to address systematic over-forecasting, especially during non-epidemic periods.\n\n**Conclusion:**  \nThe demand characteristics are well-explained by known business and external factors. These insights will guide targeted checks and model enhancements in the next phase of the data quality framework.","metadata":{"_uuid":"2c293d7a-0eb8-4e94-ae0d-0ad657067943","_cell_guid":"30d49db4-347a-4848-a014-3a9aa751cf55","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step E: Final Summary and Documentation\n# Purpose: To aggregate and summarize insights from all previous steps, document actions taken, and prepare for reporting.\n# Outputs: A collection of Markdown tables and summaries outlining the entire data quality process.\n# Rationale: This step provides a comprehensive, high-level overview of the data preparation journey, ensuring\n#            transparency and reproducibility for stakeholders and future analysis.\n# ==========================================================================\n\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\n# --- 1. Data Quality Checks and Cleaning (Steps 1 & 2) ---\nsummary = {\n    'Step': ['1. Data Quality Checks', '2. Data Cleaning'],\n    'Description': [\n        'Performed checks for missing values, data types, duplicates, negative values, and logical inconsistencies.',\n        'Corrected identified issues: filled missing values, fixed negative inventory, ensured category consistency, and addressed value anomalies.'\n    ],\n    'Key Findings': [\n        'Identified and logged issues such as negative inventory, sales > inventory, negative prices, and missing values.',\n        'Applied corrections to ensure data integrity and consistency.'\n    ]\n}\n\ndf_summary = pd.DataFrame(summary)\ndisplay(df_summary)\n\n# --- 2. Demand Characteristics (Part C) ---\ndisplay(Markdown('''\n### Demand Characteristics (Part C) – Key Insights\n\n- **Epidemic (COVID-19) periods:** Significantly lower demand, as expected.\n- **Promotions:** Effectively increase demand.\n- **Regional demand:** Stable, with minor variations and some outliers.\n- **Zero-demand rows:** Rare; intermittent demand is not a major issue.\n- **Forecast accuracy:** Forecasts tend to over-predict actual sales, but correlation is strong.\n- **No major data quality issues were found.**\n'''))\n\n# --- 3. Insights-to-Action Mapping (Part 3) ---\ndisplay(Markdown('''\n## Insights-to-Action Mapping (Part 3)\n\n| Finding                | Impact                              | Action                                      |\n|------------------------|-------------------------------------|---------------------------------------------|\n| Epidemic demand spikes | Non-stationary time series          | Add shock indicator features to models      |\n| Promotion periods      | Increased demand                    | Include promotion flag as a feature         |\n| Regional sales clusters| Geographic bias                     | Add region dummy variables                  |\n| Zero-demand rows       | Intermittent demand pattern         | Sample-check for accuracy; use intermittent demand models if needed |\n| Forecast over-forecasting | Systematic error, excess inventory risk | Review and refine forecasting methodology   |\n'''))\n\n# --- 4. Target Outputs (Part 4) ---\ndisplay(Markdown('''\n## Target Outputs (Part 4)\n\n- **Data Quality Scorecard:** Summary metrics for each quality dimension.\n- **Visual Dashboards:** Key findings visualized for stakeholders.\n- **Priority Cleaning Tasks:** Roadmap for addressing identified issues.\n- **Model Compatibility Assessment:** Evaluation of data suitability for different forecasting models.\n- **Feature Engineering Roadmap:** Plan for creating derived features (e.g., region dummies, event flags).\n- **Model Selection Justification:** Documentation of model choices based on data characteristics.\n'''))\n\n# --- 5. Next Steps ---\ndisplay(Markdown('''\n## Next Steps\n\n- **Prepare visual dashboards and scorecards.**\n- **Implement feature engineering steps (region dummies, event flags).**\n- **Refine forecasting methodology based on identified patterns.**\n- **Document model selection and justification.**\n'''))\n\nprint(\"=\"*60)\nprint(\"Data Quality Process Summary and Documentation Complete\")\nprint(\"=\"*60)","metadata":{"_uuid":"32c35c2e-ec40-48a1-b56a-a00353cf9e8a","_cell_guid":"4257a37c-603b-4fff-afb0-716463d67caa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-14T13:40:51.243618Z","iopub.execute_input":"2025-07-14T13:40:51.244296Z","iopub.status.idle":"2025-07-14T13:40:51.262980Z","shell.execute_reply.started":"2025-07-14T13:40:51.244271Z","shell.execute_reply":"2025-07-14T13:40:51.262053Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"_uuid":"cda795c9-af8d-4c00-80a8-727735519664","_cell_guid":"98ba9f5b-290c-44d0-9dbf-83871216d04d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ==========================================================================\n# Step F: Data Quality Scorecard for Model Selection\n# Purpose: To create a final scorecard that summarizes all data quality checks, corrections, and key demand\n#          characteristics to inform and justify model selection.\n# Outputs: A formatted Markdown scorecard presenting key metrics and qualitative factors.\n# Rationale: This scorecard serves as a transparent, data-driven foundation for making modeling decisions,\n#            linking the quality of the data directly to the forecasting strategy.\n# ==========================================================================\n\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\n# --- 1. Data Quality Metrics (Pull from previous steps) ---\n# (Replace with your actual metrics or calculations as needed)\n\nmetrics = {\n    'Total Rows': len(df),\n    'Missing Values': df.isnull().sum().sum(),\n    'Negative Inventory': (df['Inventory Level'] < 0).sum(),\n    'Sales > Inventory': (df['Units Sold'] > df['Inventory Level']).sum(),\n    'Negative Prices': (df['Price'] < 0).sum(),\n    'Zero-Demand Rows': (df['Units Sold'] == 0).sum(),\n    'Zero-Demand Percentage': (df['Units Sold'] == 0).mean() * 100\n}\n\n# --- 2. Demand Characteristics (Pull from previous steps) ---\ndemand_stats = {\n    'Mean Units Sold': df['Units Sold'].mean(),\n    'Std. Units Sold': df['Units Sold'].std(),\n    'Epidemic Impact (Avg Units Sold)': df[df['Epidemic']]['Units Sold'].mean(),\n    'Non-Epidemic Impact (Avg Units Sold)': df[~df['Epidemic']]['Units Sold'].mean(),\n    'Promotion Impact (Avg Units Sold)': df[df['Promotion']]['Units Sold'].mean(),\n    'Non-Promotion Impact (Avg Units Sold)': df[~df['Promotion']]['Units Sold'].mean(),\n    'Forecast Correlation (vs. Actual)': df['Demand'].corr(df['Units Sold']) if 'Demand' in df.columns else 'N/A'\n}\n\n# --- 3. Label Corrections (Tracked during cleaning) ---\nlabel_corrections = {\n    'Region': {\n        'North ': 'North',\n        'South ': 'South'\n    },\n    'Category': {\n        'Electronics': 'Electronics/Appliances',\n        'Clothing': 'Apparel'\n    }\n}\n\n# --- 4. Model Selection Factors ---\nmodel_factors = [\n    'Epidemic (COVID-19) periods: Accounted for as external shocks',\n    'Promotion periods: Included as features',\n    'Regional effects: Regional dummy variables added',\n    'Zero-demand patterns: Intermittent demand models considered if needed',\n    'Forecast accuracy: Correlation with actual sales is strong, but over-forecasting noted',\n    'Label consistency: Region and category labels standardized'\n]\n\n# --- 5. Scorecard Presentation ---\n\ndisplay(Markdown('''\n# Data Quality Scorecard for Model Selection\n\n---\n\n## 1. Data Quality Metrics\n\n| Metric                  | Value                |\n|-------------------------|----------------------|\n| Total Rows              | {:,}                |\n| Missing Values          | {:,}                |\n| Negative Inventory      | {:,}                |\n| Sales > Inventory       | {:,}                |\n| Negative Prices         | {:,}                |\n| Zero-Demand Rows        | {:,}                |\n| Zero-Demand Percentage  | {:.2f}%             |\n\n---\n\n## 2. Demand Characteristics\n\n| Aspect                                | Value                |\n|----------------------------------------|----------------------|\n| Mean Units Sold                       | {:.2f}              |\n| Std. Units Sold                       | {:.2f}              |\n| Epidemic Impact (Avg Units Sold)      | {:.2f}              |\n| Non-Epidemic Impact (Avg Units Sold)  | {:.2f}              |\n| Promotion Impact (Avg Units Sold)     | {:.2f}              |\n| Non-Promotion Impact (Avg Units Sold) | {:.2f}              |\n| Forecast Correlation (vs. Actual)     | {:.2f}              |\n\n---\n\n## 3. Label Corrections\n\n**Summary of changes made to region and category labels:**\n'''.format(\n    metrics['Total Rows'],\n    metrics['Missing Values'],\n    metrics['Negative Inventory'],\n    metrics['Sales > Inventory'],\n    metrics['Negative Prices'],\n    metrics['Zero-Demand Rows'],\n    metrics['Zero-Demand Percentage'],\n    demand_stats['Mean Units Sold'],\n    demand_stats['Std. Units Sold'],\n    demand_stats['Epidemic Impact (Avg Units Sold)'],\n    demand_stats['Non-Epidemic Impact (Avg Units Sold)'],\n    demand_stats['Promotion Impact (Avg Units Sold)'],\n    demand_stats['Non-Promotion Impact (Avg Units Sold)'],\n    demand_stats['Forecast Correlation (vs. Actual)']\n)))\n\nfor field, changes in label_corrections.items():\n    display(Markdown(f'- **{field}**:'))\n    for old, new in changes.items():\n        display(Markdown(f'  - `{old}` → `{new}`'))\n\ndisplay(Markdown('''\n---\n\n## 4. Model Selection Factors\n\n**Key factors considered for model selection:**\n'''))\nfor factor in model_factors:\n    display(Markdown(f'- {factor}'))\n\ndisplay(Markdown('''\n---\n\n## 5. Next Steps\n\n- **Prepare visual dashboards and scorecards.**\n- **Implement feature engineering steps (region dummies, event flags).**\n- **Refine forecasting methodology based on identified patterns.**\n- **Document model selection and justification.**\n'''))\n\nprint(\"=\"*60)\nprint(\"Data Quality Scorecard for Model Selection Complete\")\nprint(\"=\"*60)","metadata":{"_uuid":"ddf3bb70-56a9-42a5-a09f-1ea66637f53e","_cell_guid":"6f62e783-9288-470c-8955-d0cd044315eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-14T13:40:51.264168Z","iopub.execute_input":"2025-07-14T13:40:51.264752Z","iopub.status.idle":"2025-07-14T13:40:51.323356Z","shell.execute_reply.started":"2025-07-14T13:40:51.264722Z","shell.execute_reply":"2025-07-14T13:40:51.322575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model Selection and Justification**\n\n---\n\n## 1. Objective\n\nSelect a demand forecasting model that:\n\n- Captures **daily, product-level demand dynamics** across stores\n- Incorporates **temporal features**, **event indicators**, and **cross-sectional interactions**\n- Models \"black swan\" disruptions (e.g., **COVID-19**) via an `Epidemic` event flag\n- Enables **backtesting-based inventory simulation** to assess policy improvements\n\n---\n\n## 2. Temporal and Structural Context\n\n**Rationale:**  \n- **Panel time series**: Multiple grouped series (`Store ID` × `Product ID`)  \n- **Daily granularity**: Supports modeling short-term volatility and seasonal cycles  \n- **Epidemic period**: Binary `Epidemic` flag marks COVID-19–affected days  \n- **Exogenous context**: Features such as `Weather`, `Promotion`, `Competitor Pricing`\n\n---\n\n## 3. Industry Case Study Insights\n\n| Source        | Insight                                                             |\n|---------------|----------------------------------------------------------------------|\n| **Target**    | Modeled pandemic disruptions via event-aware AI, reduced overstock by ~$2B |\n| **Walmart**   | Used lag-based, event-enriched forecasting to balance availability vs overstock |\n| **JD.com**    | Dynamically retrained models on COVID-affected segments, improved shock resilience |\n| **Quantzig**  | Achieved 30%+ accuracy improvement, 20% lower carrying cost using demand simulations |\n| **Academia**  | Pandemic-aware LSTM and boosting models consistently outperformed static baselines |\n\n---\n\n## 4. Candidate Models\n\n| Model                          | Strengths                                                | Limitations                                |\n|--------------------------------|----------------------------------------------------------|--------------------------------------------|\n| ARIMA/SARIMAX                  | Seasonality, trend modeling                              | Weak for multivariate and black swan events|\n| Prophet                        | Easy event modeling                                      | Not suitable for high-dimensional panels   |\n| RNN / LSTM                     | Captures nonlinear time dependencies                     | Complex, opaque, requires more training data |\n| TFT (Temporal Fusion Transformer)| Event-aware, interpretable deep model                  | Computationally intensive, complex tuning  |\n| **LightGBM + Temporal Features** | Fast, interpretable, supports categorical + event data  | Requires manual lag/rolling feature design |\n\n---\n\n## 5. Chosen Model: **LightGBM with Temporal and Event Features**\n\n| Justification                             | Notes                                                               |\n|-------------------------------------------|---------------------------------------------------------------------|\n| Panel-ready                               | Models demand across `Store ID` × `Product ID` simultaneously       |\n| Explicit event modeling                   | Includes `Epidemic`, `Promotion`, `Weather`, and seasonality flags |\n| Handles structured data efficiently       | Optimized for 76,000 rows, mixed-type variables                     |\n| Lag and rolling features supported        | Manual construction enables demand memory and volatility smoothing |\n| Interpretable via SHAP values             | Aids post-modeling diagnostics and business communication           |\n| Suitable for simulation and backtesting   | Supports scenario-based inventory optimization                     |\n\n---\n\n## 6. Forecasting Strategy\n\n- **Lag Features:** Prior `Units Sold`, `Inventory Level`, `Units Ordered` (t−1, t−7, t−14, t−30)\n- **Rolling Averages:** Trailing 7/14/30-day demand windows\n- **Temporal Indicators:** `Day of Week`, `Month`, `Seasonality`, `Holiday`\n- **Event Flags:** `Epidemic`, `Promotion`, `Weather Condition`\n- **Group-Based Encoding:** Mean historical demand by `Store ID` and `Product ID`\n\n---\n\n## 7. Hypothesis\n\n> \"A LightGBM-based pipeline augmented with temporal lags, rolling windows, and explicit event indicators will outperform static models by learning cross-store product behavior, capturing COVID-era volatility, and supporting actionable backtesting for inventory adjustment.\"\n\n---\n\n## 8. Next Steps\n\n1. **Feature engineering**: Lagged, rolling, and time-derived variables  \n2. **Model training**: LightGBM with time-aware cross-validation  \n3. **Validation**: Error diagnostics by event type (pandemic vs. normal)  \n4. **Simulation**: Backtest inventory scenarios using forecasted `Units Sold`  \n5. **Policy optimization**: Identify reorder point changes to reduce stockouts and overstock\n\n---","metadata":{"_uuid":"4700cdd4-3eb5-4172-be05-1c798e1cd7bd","_cell_guid":"0335ec1d-af79-40cc-8163-1fba77eac887","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-14T08:24:51.567865Z","iopub.execute_input":"2025-07-14T08:24:51.568267Z","iopub.status.idle":"2025-07-14T08:24:51.573247Z","shell.execute_reply.started":"2025-07-14T08:24:51.568242Z","shell.execute_reply":"2025-07-14T08:24:51.572058Z"}}},{"cell_type":"code","source":"## Cell 1: Sorting and Setup\n\n1. Sort the dataset by Store ID, Product ID, and Date to ensure consistent temporal ordering before creating time-based features.\n\n","metadata":{"_uuid":"c56d85c1-46ec-497a-9dcf-35dc9b0c82a8","_cell_guid":"a839a7a1-0f4c-4622-b987-fbba0ce7a365","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-14T13:40:51.324240Z","iopub.execute_input":"2025-07-14T13:40:51.324543Z","iopub.status.idle":"2025-07-14T13:40:51.328582Z","shell.execute_reply.started":"2025-07-14T13:40:51.324515Z","shell.execute_reply":"2025-07-14T13:40:51.327760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Cell 2: Lag Features\n\n2. Generate lag features for Units Sold using shifts of 1, 7, and 14 days grouped by Store ID and Product ID.\n\n3. Generate lag features for Inventory Level and Units Ordered using a 1-day shift grouped by Store ID and Product ID.\n\n7. Generate a lagged version of the Stockout flag with a 1-day shift grouped by Store ID and Product ID.\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Cell 3: Rolling Window Features\n\n4. Compute 7-day rolling mean and standard deviation of Units Sold grouped by Store ID and Product ID, excluding the current day to avoid leakage.\n\n5. Compute 14-day and 30-day rolling statistics for Units Sold and Inventory Level, including max and min demand windows as needed.\n\n12. Calculate a 14-day rolling sum of the Promotion flag grouped by Store ID and Product ID to capture promotional intensity.\n\n13. Calculate a 7-day rolling sum of the Epidemic flag grouped by Store ID and Product ID to quantify local pandemic influence.\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Cell 4: Price and Stockout Features\n\n6. Create a binary Stockout flag set to 1 if Inventory Level is zero, else 0.\n\n8. Calculate Effective Price as Price multiplied by (1 - Discount / 100).\n\n9. Compute the difference between Effective Price and Competitor Pricing to capture pricing sensitivity.\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Cell 5: Temporal Features\n\n10. Extract temporal features from the Date index: Day of Week, Month, and a binary IsWeekend flag based on Day of Week.\n\n11. Merge in a holiday calendar and generate a binary IsHoliday flag to mark special retail days.\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Cell 6: Group-Based Target Encodings\n\n14. Compute the global average Units Sold per Product ID across all stores and merge this as AvgProductSales_global.\n\n15. Compute the global average Units Sold per Store ID across all products and merge this as AvgStoreSales_global.\n\n16. Group by Category and Region to compute the mean Units Sold per Category per Region and merge this as AvgCategorySales_by_Region.\n\n17. Perform mean encoding for Product ID and Store ID based on historical average Units Sold grouped by date or period as appropriate.\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Cell 7: Finalization and Validation\n\n18. Review all new features to confirm correct alignment with each Store ID, Product ID, and Date.\n\n19. Downcast all new numeric columns to float32 or uint16 where appropriate to optimize memory usage.\n\n20. Save the engineered dataset to file or cache it for training, ensuring that no future data is leaked into any rows used for model input.\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model training","metadata":{"_uuid":"8a1d5b06-0608-438a-93a9-2968d1235a2a","_cell_guid":"78b3fffd-6fe9-4221-95be-c0e677df4af3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-14T13:40:51.329535Z","iopub.execute_input":"2025-07-14T13:40:51.329882Z","iopub.status.idle":"2025-07-14T13:40:51.344499Z","shell.execute_reply.started":"2025-07-14T13:40:51.329853Z","shell.execute_reply":"2025-07-14T13:40:51.343482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Validation","metadata":{"_uuid":"61bdc8ca-c4c3-4b61-85ac-7997ed8d8687","_cell_guid":"cde34d25-3fd8-4347-ba8a-6e2c01d0fb84","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-14T13:40:51.345393Z","iopub.execute_input":"2025-07-14T13:40:51.345680Z","iopub.status.idle":"2025-07-14T13:40:51.360112Z","shell.execute_reply.started":"2025-07-14T13:40:51.345658Z","shell.execute_reply":"2025-07-14T13:40:51.359292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simulation","metadata":{"_uuid":"aa097d73-899d-452c-aea8-d3dc3f93b5dc","_cell_guid":"d679ad46-6fa0-4dd2-b46a-bf9f487b2d0c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-14T13:40:51.361311Z","iopub.execute_input":"2025-07-14T13:40:51.361580Z","iopub.status.idle":"2025-07-14T13:40:51.376552Z","shell.execute_reply.started":"2025-07-14T13:40:51.361556Z","shell.execute_reply":"2025-07-14T13:40:51.375429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Policy optimization","metadata":{"_uuid":"9a9ed3bf-a488-4013-a5d2-70f5c050b5a3","_cell_guid":"be39aa12-295c-42fd-8418-b0a4567fb23e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-14T13:40:51.377526Z","iopub.execute_input":"2025-07-14T13:40:51.377866Z","iopub.status.idle":"2025-07-14T13:40:51.391986Z","shell.execute_reply.started":"2025-07-14T13:40:51.377837Z","shell.execute_reply":"2025-07-14T13:40:51.390982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"_uuid":"6445c86e-e9ce-41fc-8490-18663b0e5886","_cell_guid":"e2de6ae3-f804-44c2-b2c1-452437d8c2fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}