{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe1077d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-05T14:18:58.804507Z",
     "iopub.status.busy": "2025-06-05T14:18:58.804187Z",
     "iopub.status.idle": "2025-06-05T14:19:00.926043Z",
     "shell.execute_reply": "2025-06-05T14:19:00.924984Z"
    },
    "papermill": {
     "duration": 2.129393,
     "end_time": "2025-06-05T14:19:00.927668",
     "exception": false,
     "start_time": "2025-06-05T14:18:58.798275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/retail-store-inventory-and-demand-forecasting/sales_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4672d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:24:45.206197Z",
     "iopub.status.busy": "2025-06-05T13:24:45.205776Z",
     "iopub.status.idle": "2025-06-05T13:24:45.226916Z",
     "shell.execute_reply": "2025-06-05T13:24:45.225440Z",
     "shell.execute_reply.started": "2025-06-05T13:24:45.206160Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.003248,
     "end_time": "2025-06-05T14:19:00.934970",
     "exception": false,
     "start_time": "2025-06-05T14:19:00.931722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Data Import & Cleaning Hypotheses**\n",
    "\n",
    "### **1. Import-Time Auto-Corrections**  \n",
    "*(Automatically handled during data loading)*  \n",
    "\n",
    "| **Issue Type**               | **Correction Applied**          | **Preserved Rows** | **Impact Analysis**                   |\n",
    "|-------------------------------|----------------------------------|--------------------|---------------------------------------|\n",
    "| Negative Inventory            | Clip to 0                       | Yes                | May mask true stockouts               |\n",
    "| Sales > Inventory             | Cap at Inventory Level          | Yes                | Preserves volume but loses overflow   |\n",
    "| Invalid Dates                 | Drop rows                       | No                 | Creates time-series gaps              |\n",
    "| Numeric Overflow              | Downcast to unsigned            | Yes                | Reduces memory usage                  |\n",
    "\n",
    "**Hypothesis**:  \n",
    "> \"Automated corrections during import will preserve 98%+ of rows while creating traceable artifacts for analysis.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Post-Import Quality Checks**  \n",
    "*(Require manual validation)*  \n",
    "\n",
    "| **Check Type**          | **Detection Method**             | **Potential Actions**                  |\n",
    "|-------------------------|-----------------------------------|----------------------------------------|\n",
    "| Label Consistency       | Fuzzy matching on text fields    | Standardize categories/regions         |\n",
    "| Temporal Gaps           | Missing date detection           | Interpolate or flag as special events  |\n",
    "| Price Inflation         | 3Ïƒ deviation from product history| Verify true inflation vs data errors   |\n",
    "| Promotion Efficacy      | Lift analysis (promo vs non-promo)| Filter phantom demand from stockouts   |\n",
    "\n",
    "**Hypothesis**:  \n",
    "> \"5-15% of rows will require post-import adjustments, mainly in categorical labels and promotional periods.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Feature Engineering Implications**\n",
    "\n",
    "| **Observed Pattern**       | **Feature Design**               | **Rationale**                         |\n",
    "|----------------------------|-----------------------------------|---------------------------------------|\n",
    "| Frequent zero-demand       | Demand presence indicator        | Supports intermittent-demand models   |\n",
    "| Epidemic spikes            | Shock absorption features        | Isolates exceptional events           |\n",
    "| Regional price variance    | Region-price clusters            | Captures local market dynamics        |\n",
    "\n",
    "**Hypothesis**:  \n",
    "> \"Auto-corrected inventory/sales relationships will require demand reconstruction features.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7824d07",
   "metadata": {
    "papermill": {
     "duration": 0.003101,
     "end_time": "2025-06-05T14:19:00.941509",
     "exception": false,
     "start_time": "2025-06-05T14:19:00.938408",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f80eb7",
   "metadata": {
    "papermill": {
     "duration": 0.003117,
     "end_time": "2025-06-05T14:19:00.947912",
     "exception": false,
     "start_time": "2025-06-05T14:19:00.944795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5651ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T14:19:00.956164Z",
     "iopub.status.busy": "2025-06-05T14:19:00.955552Z",
     "iopub.status.idle": "2025-06-05T14:19:00.961702Z",
     "shell.execute_reply": "2025-06-05T14:19:00.960885Z"
    },
    "papermill": {
     "duration": 0.011845,
     "end_time": "2025-06-05T14:19:00.963092",
     "exception": false,
     "start_time": "2025-06-05T14:19:00.951247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "DATA_PATH = Path('/kaggle/input/retail-store-inventory-and-demand-forecasting/sales_data.csv')\n",
    "CHUNKSIZE = 50_000\n",
    "DATE_COL = 'Date'\n",
    "\n",
    "# Optimized dtypes\n",
    "DTYPES = {\n",
    "    'Store ID': 'category',\n",
    "    'Product ID': 'category',\n",
    "    'Category': 'category',\n",
    "    'Region': 'category',\n",
    "    'Inventory Level': 'uint16',\n",
    "    'Units Sold': 'uint16',\n",
    "    'Units Ordered': 'uint16',\n",
    "    'Price': 'float32',\n",
    "    'Discount': 'float32',\n",
    "    'Weather Condition': 'category',\n",
    "    'Promotion': 'bool',\n",
    "    'Competitor Pricing': 'float32',\n",
    "    'Seasonality': 'category',\n",
    "    'Epidemic': 'bool',\n",
    "    'Demand': 'uint16'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aef4b04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T14:19:00.971152Z",
     "iopub.status.busy": "2025-06-05T14:19:00.970833Z",
     "iopub.status.idle": "2025-06-05T14:19:01.625998Z",
     "shell.execute_reply": "2025-06-05T14:19:01.625212Z"
    },
    "papermill": {
     "duration": 0.660844,
     "end_time": "2025-06-05T14:19:01.627477",
     "exception": false,
     "start_time": "2025-06-05T14:19:00.966633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported 76,000 rows (out of 76,000 total)\n",
      "\n",
      "Rows dropped during processing:\n",
      "- Date Parsing: 0 rows\n",
      "- Inventory Adjustment: 0 rows\n",
      "- Sales Adjustment: 0 rows\n",
      "- Dtype Conversion: 0 rows\n",
      "- Chunk Failures: 0 rows\n",
      "\n",
      "Rows auto-corrected (not dropped):\n",
      "- Negative Inventory: 0 rows\n",
      "- Sales Exceeding Inventory: 0 rows\n",
      "- Invalid Dates: 0 rows\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store ID</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Region</th>\n",
       "      <th>Inventory Level</th>\n",
       "      <th>Units Sold</th>\n",
       "      <th>Units Ordered</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Weather Condition</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>Competitor Pricing</th>\n",
       "      <th>Seasonality</th>\n",
       "      <th>Epidemic</th>\n",
       "      <th>Demand</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>S001</td>\n",
       "      <td>P0001</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>North</td>\n",
       "      <td>195</td>\n",
       "      <td>102</td>\n",
       "      <td>252</td>\n",
       "      <td>72.720001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Snowy</td>\n",
       "      <td>False</td>\n",
       "      <td>85.730003</td>\n",
       "      <td>Winter</td>\n",
       "      <td>False</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>S001</td>\n",
       "      <td>P0002</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>North</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>249</td>\n",
       "      <td>80.160004</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Snowy</td>\n",
       "      <td>True</td>\n",
       "      <td>92.019997</td>\n",
       "      <td>Winter</td>\n",
       "      <td>False</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>S001</td>\n",
       "      <td>P0003</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>North</td>\n",
       "      <td>247</td>\n",
       "      <td>114</td>\n",
       "      <td>612</td>\n",
       "      <td>62.939999</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Snowy</td>\n",
       "      <td>True</td>\n",
       "      <td>60.080002</td>\n",
       "      <td>Winter</td>\n",
       "      <td>False</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>S001</td>\n",
       "      <td>P0004</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>North</td>\n",
       "      <td>139</td>\n",
       "      <td>45</td>\n",
       "      <td>102</td>\n",
       "      <td>87.629997</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Snowy</td>\n",
       "      <td>False</td>\n",
       "      <td>85.190002</td>\n",
       "      <td>Winter</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>S001</td>\n",
       "      <td>P0005</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>North</td>\n",
       "      <td>152</td>\n",
       "      <td>65</td>\n",
       "      <td>271</td>\n",
       "      <td>54.410000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Snowy</td>\n",
       "      <td>False</td>\n",
       "      <td>51.630001</td>\n",
       "      <td>Winter</td>\n",
       "      <td>False</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Store ID Product ID     Category Region  Inventory Level  \\\n",
       "Date                                                                  \n",
       "2022-01-01     S001      P0001  Electronics  North              195   \n",
       "2022-01-01     S001      P0002     Clothing  North              117   \n",
       "2022-01-01     S001      P0003     Clothing  North              247   \n",
       "2022-01-01     S001      P0004  Electronics  North              139   \n",
       "2022-01-01     S001      P0005    Groceries  North              152   \n",
       "\n",
       "            Units Sold  Units Ordered      Price  Discount Weather Condition  \\\n",
       "Date                                                                           \n",
       "2022-01-01         102            252  72.720001       5.0             Snowy   \n",
       "2022-01-01         117            249  80.160004      15.0             Snowy   \n",
       "2022-01-01         114            612  62.939999      10.0             Snowy   \n",
       "2022-01-01          45            102  87.629997      10.0             Snowy   \n",
       "2022-01-01          65            271  54.410000       0.0             Snowy   \n",
       "\n",
       "            Promotion  Competitor Pricing Seasonality  Epidemic  Demand  \n",
       "Date                                                                     \n",
       "2022-01-01      False           85.730003      Winter     False     115  \n",
       "2022-01-01       True           92.019997      Winter     False     229  \n",
       "2022-01-01       True           60.080002      Winter     False     157  \n",
       "2022-01-01      False           85.190002      Winter     False      52  \n",
       "2022-01-01      False           51.630001      Winter     False      59  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 2 Import Process with Detailed Tracking\n",
    "\n",
    "def import_retail_data():\n",
    "    chunks = []\n",
    "    error_log = []\n",
    "    stats = {\n",
    "        'total_rows': 0,\n",
    "        'rows_dropped': {\n",
    "            'date_parsing': 0,\n",
    "            'inventory_adjustment': 0,\n",
    "            'sales_adjustment': 0,\n",
    "            'dtype_conversion': 0,\n",
    "            'chunk_failures': 0\n",
    "        },\n",
    "        # NEW: Track auto-corrected values\n",
    "        'rows_modified': {\n",
    "            'negative_inventory': 0,\n",
    "            'sales_exceeding_inventory': 0,\n",
    "            'invalid_dates': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with pd.read_csv(\n",
    "            DATA_PATH,\n",
    "            chunksize=CHUNKSIZE,\n",
    "            dtype=DTYPES,\n",
    "            parse_dates=[DATE_COL],\n",
    "            on_bad_lines='warn',\n",
    "            encoding='utf-8'\n",
    "        ) as reader:\n",
    "            \n",
    "            for chunk_idx, chunk in enumerate(reader):\n",
    "                stats['total_rows'] += len(chunk)\n",
    "                try:\n",
    "                    # Validate required columns\n",
    "                    required_cols = set(DTYPES.keys())\n",
    "                    missing_cols = required_cols - set(chunk.columns)\n",
    "                    if missing_cols:\n",
    "                        raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "                    \n",
    "                    # Track original state\n",
    "                    original_negative_inv = (chunk['Inventory Level'] < 0).sum()\n",
    "                    original_over_sales = (chunk['Units Sold'] > chunk['Inventory Level']).sum()\n",
    "                    \n",
    "                    # Clean data - Date parsing\n",
    "                    chunk[DATE_COL] = pd.to_datetime(chunk[DATE_COL], errors='coerce')\n",
    "                    date_dropped = chunk[DATE_COL].isna().sum()\n",
    "                    chunk = chunk.dropna(subset=[DATE_COL])\n",
    "                    stats['rows_dropped']['date_parsing'] += date_dropped\n",
    "                    stats['rows_modified']['invalid_dates'] += date_dropped  # NEW\n",
    "                    \n",
    "                    # Clean data - Inventory adjustment\n",
    "                    chunk['Inventory Level'] = chunk['Inventory Level'].clip(lower=0)\n",
    "                    stats['rows_modified']['negative_inventory'] += original_negative_inv  # NEW\n",
    "                    stats['rows_dropped']['inventory_adjustment'] += original_negative_inv\n",
    "                    \n",
    "                    # Clean data - Sales adjustment\n",
    "                    chunk['Units Sold'] = np.where(\n",
    "                        chunk['Units Sold'] > chunk['Inventory Level'],\n",
    "                        chunk['Inventory Level'],\n",
    "                        chunk['Units Sold']\n",
    "                    )\n",
    "                    stats['rows_modified']['sales_exceeding_inventory'] += original_over_sales  # NEW\n",
    "                    stats['rows_dropped']['sales_adjustment'] += original_over_sales\n",
    "                    \n",
    "                    # Handle numeric overflows (unchanged)\n",
    "                    for col in ['Inventory Level', 'Units Sold', 'Units Ordered']:\n",
    "                        try:\n",
    "                            chunk[col] = pd.to_numeric(chunk[col], downcast='unsigned')\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    chunks.append(chunk)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    stats['rows_dropped']['chunk_failures'] += len(chunk)\n",
    "                    error_log.append(f\"Chunk {chunk_idx} failed: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if not chunks:\n",
    "            raise ValueError(\"No valid data chunks were processed\")\n",
    "            \n",
    "        # Combine and finalize (unchanged)\n",
    "        df = pd.concat(chunks, ignore_index=False)\n",
    "        \n",
    "        # Final dtype enforcement (unchanged)\n",
    "        for col, dtype in DTYPES.items():\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(dtype)\n",
    "                except Exception as e:\n",
    "                    error_log.append(f\"Dtype conversion failed for {col}: {str(e)}\")\n",
    "                    mask = pd.to_numeric(df[col], errors='coerce').isna()\n",
    "                    stats['rows_dropped']['dtype_conversion'] += mask.sum()\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        df = df.set_index(DATE_COL).sort_index()\n",
    "        stats['final_rows'] = len(df)\n",
    "        \n",
    "        return df, error_log, stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_log.append(f\"Fatal import error: {str(e)}\")\n",
    "        return None, error_log, stats\n",
    "\n",
    "\n",
    "# Execute and Display Enhanced Results (Updated Output)\n",
    "df, import_errors, import_stats = import_retail_data()\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Successfully imported {import_stats['final_rows']:,} rows (out of {import_stats['total_rows']:,} total)\")\n",
    "    \n",
    "    print(\"\\nRows dropped during processing:\")\n",
    "    for reason, count in import_stats['rows_dropped'].items():\n",
    "        print(f\"- {reason.replace('_', ' ').title()}: {count:,} rows\")\n",
    "    \n",
    "    # NEW: Display auto-corrected counts\n",
    "    print(\"\\nRows auto-corrected (not dropped):\")\n",
    "    for issue, count in import_stats['rows_modified'].items():\n",
    "        print(f\"- {issue.replace('_', ' ').title()}: {count:,} rows\")\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"Import failed\")\n",
    "\n",
    "if import_errors:\n",
    "    print(\"\\nEncountered warnings/errors:\")\n",
    "    for i, error in enumerate(import_errors, 1):\n",
    "        print(f\"{i}. {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb845b2",
   "metadata": {
    "papermill": {
     "duration": 0.003491,
     "end_time": "2025-06-05T14:19:01.635155",
     "exception": false,
     "start_time": "2025-06-05T14:19:01.631664",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c0cf47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:24:19.501951Z",
     "iopub.status.busy": "2025-06-05T13:24:19.501474Z",
     "iopub.status.idle": "2025-06-05T13:24:19.528753Z",
     "shell.execute_reply": "2025-06-05T13:24:19.527008Z",
     "shell.execute_reply.started": "2025-06-05T13:24:19.501921Z"
    },
    "papermill": {
     "duration": 0.003381,
     "end_time": "2025-06-05T14:19:01.642213",
     "exception": false,
     "start_time": "2025-06-05T14:19:01.638832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Retail Data Quality Assessment Framework**\n",
    "\n",
    "### **1. Objectives**\n",
    "This analysis establishes a quality baseline for:\n",
    "- **Forecast Reliability**: Identify data integrity issues impacting predictions\n",
    "- **Pattern Discovery**: Uncover demand, sales, and inventory relationships\n",
    "- **Model Guidance**: Determine appropriate modeling approaches\n",
    "- **Metadata Documentation**: Create reproducible analysis standards\n",
    "\n",
    "### **2. Assessment Dimensions**\n",
    "\n",
    "#### **A. Core Data Validation**\n",
    "**Purpose**: Ensure fundamental data integrity  \n",
    "**Key Checks**:\n",
    "1. **Label Consistency**  \n",
    "   - Category naming conventions (e.g., \"Electronics\" standardization)\n",
    "   - Weather condition labels (e.g., validate \"Rain\" vs \"Raining\")\n",
    "\n",
    "2. **Temporal Integrity**  \n",
    "   - Complete date sequences without gaps\n",
    "   - Logical timestamp ranges (no future dates or ancient records)\n",
    "\n",
    "3. **Value Validation**  \n",
    "   - Demand/sales within plausible ranges\n",
    "   - Price and discount logical relationships\n",
    "\n",
    "#### **B. Dataset Composition**\n",
    "**Purpose**: Understand data structure suitability  \n",
    "**Key Metrics**:\n",
    "| Dimension        | Analysis Purpose                          | Validation Approach              |\n",
    "|------------------|------------------------------------------|----------------------------------|\n",
    "| Products         | Forecast granularity determination       | SKU-level vs category analysis   |\n",
    "| Time Coverage    | Seasonality assessment                   | 2+ years for robust patterns     |\n",
    "| Locations        | Geographic representation evaluation     | Store-region alignment check     |\n",
    "\n",
    "#### **C. Demand Analysis**\n",
    "**Purpose**: Identify modeling requirements  \n",
    "**Critical Examinations**:\n",
    "- **Value Distribution**: Detection of artificial censoring\n",
    "- **Zero-Inflation**: Intermittent demand pattern analysis\n",
    "- **Event Impacts**: Epidemic and promotion effects\n",
    "\n",
    "### **3. Actionable Insights Matrix**\n",
    "\n",
    "| Data Quality Finding      | Analytical Impact                  | Recommended Resolution           |\n",
    "|--------------------------|-----------------------------------|----------------------------------|\n",
    "| Frequent zero-demand     | Intermittent demand pattern       | Croston's/TSB models            |\n",
    "| Regional sales variance  | Geographic bias                   | Regional fixed effects          |\n",
    "| Epidemic demand shocks   | Non-stationary time series        | Epidemic dummy variables       |\n",
    "| Weather data missingness | Incomplete contextual factors     | Regional weather imputation     |\n",
    "\n",
    "### **4. Implementation Deliverables**\n",
    "1. **Data Quality Certification**\n",
    "   - Priority cleaning roadmap\n",
    "   - Data suitability grading\n",
    "\n",
    "2. **Feature Development Plan**\n",
    "   - Required derived features\n",
    "   - Contextual data enhancements\n",
    "\n",
    "3. **Modeling Guidelines**\n",
    "   - Algorithm selection criteria\n",
    "   - Special case handling procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5596f",
   "metadata": {
    "papermill": {
     "duration": 0.00338,
     "end_time": "2025-06-05T14:19:01.649251",
     "exception": false,
     "start_time": "2025-06-05T14:19:01.645871",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f12736e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T14:19:01.658138Z",
     "iopub.status.busy": "2025-06-05T14:19:01.657776Z",
     "iopub.status.idle": "2025-06-05T14:19:01.805677Z",
     "shell.execute_reply": "2025-06-05T14:19:01.804634Z"
    },
    "papermill": {
     "duration": 0.154322,
     "end_time": "2025-06-05T14:19:01.807158",
     "exception": false,
     "start_time": "2025-06-05T14:19:01.652836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY REPORT ===\n",
      "\n",
      "1. Suspect Categories (0 found):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Date Issues:\n",
      "- Range: 2022-01-01 00:00:00 to 2024-01-30 00:00:00\n",
      "- Missing dates: 0\n",
      "- Pre-epidemic records: 0\n",
      "- Duplicate dates: 75240\n",
      "- Inferred frequency: None\n",
      "\n",
      "3. Demand Outliers (>234.0 or <17.0):\n",
      "- 1484 extreme values\n",
      "- Demand min: 4, max: 430\n",
      "\n",
      "4. Missing Values by Column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Count, Missing %]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Rows with any missing value: 0\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Retail Data Quality Audit â€“ Part A: Core Data Validation\n",
    "#\n",
    "# This cell performs a comprehensive audit of the raw retail data,\n",
    "# focusing on:\n",
    "#   1. Category label anomalies (short, rare, or catch-all terms)\n",
    "#   2. Date integrity (range, gaps, duplicates, pre-epidemic records)\n",
    "#   3. Demand outliers (based on configurable percentiles)\n",
    "#   4. Missing data (counts, percentages, and row-level summary)\n",
    "#\n",
    "# Results are summarized in a quality_report dictionary and displayed\n",
    "# in a format suitable for both Jupyter and script environments.\n",
    "# ================================================================\n",
    "\n",
    "# === CONFIGURABLE PARAMETERS ===\n",
    "RARE_CATEGORY_THRESHOLD = 5\n",
    "SHORT_LABEL_LENGTH = 3\n",
    "OUTLIER_PERCENTILES = [0.01, 0.99]\n",
    "EPIDEMIC_START_DATE = '2020-01-01'\n",
    "\n",
    "# === DATA QUALITY CHECKS (PART A) ===\n",
    "\n",
    "quality_report = {}\n",
    "\n",
    "try:\n",
    "    # 1. Category Label Analysis\n",
    "    cat_series = df['Category'].astype(str).str.strip()\n",
    "    category_counts = cat_series.value_counts()\n",
    "    suspect_categories = category_counts[\n",
    "        (category_counts.index.str.len() <= SHORT_LABEL_LENGTH) |\n",
    "        (category_counts < RARE_CATEGORY_THRESHOLD) |\n",
    "        (category_counts.index.str.contains('Misc|Other', case=False, regex=True))\n",
    "    ]\n",
    "    quality_report['category_issues'] = suspect_categories\n",
    "\n",
    "    # 2. Date Validation\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    duplicate_dates = df.index.duplicated().sum()\n",
    "    inferred_freq = pd.infer_freq(df.index)\n",
    "    date_range = (df.index.min(), df.index.max())\n",
    "    missing_dates = pd.date_range(start=date_range[0], end=date_range[1]).difference(df.index)\n",
    "    pre_epidemic_records = df[df.index < EPIDEMIC_START_DATE]\n",
    "    date_anomalies = {\n",
    "        'date_range': date_range,\n",
    "        'missing_dates': missing_dates,\n",
    "        'pre_epidemic_records': pre_epidemic_records,\n",
    "        'duplicate_dates': duplicate_dates,\n",
    "        'inferred_frequency': inferred_freq\n",
    "    }\n",
    "    quality_report['date_issues'] = date_anomalies\n",
    "\n",
    "    # 3. Demand Outlier Detection\n",
    "    demand_stats = df['Demand'].describe(percentiles=OUTLIER_PERCENTILES)\n",
    "    low_thresh = demand_stats[f'{int(OUTLIER_PERCENTILES[0]*100)}%']\n",
    "    high_thresh = demand_stats[f'{int(OUTLIER_PERCENTILES[1]*100)}%']\n",
    "    demand_outliers = df[\n",
    "        (df['Demand'] < low_thresh) | \n",
    "        (df['Demand'] > high_thresh)\n",
    "    ]\n",
    "    quality_report['demand_outliers'] = {\n",
    "        'stats': demand_stats,\n",
    "        'outlier_count': len(demand_outliers),\n",
    "        'min': df['Demand'].min(),\n",
    "        'max': df['Demand'].max()\n",
    "    }\n",
    "\n",
    "    # 4. Missing Data Analysis\n",
    "    missing_data = df.isna().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing %': missing_percent\n",
    "    })\n",
    "    missing_summary = missing_summary[missing_summary['Missing Count'] > 0]\n",
    "    rows_with_any_missing = df.isnull().any(axis=1).sum()\n",
    "    quality_report['missing_data'] = missing_summary\n",
    "    quality_report['rows_with_any_missing'] = rows_with_any_missing\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during data quality audit: {e}\")\n",
    "\n",
    "# === DISPLAY RESULTS ===\n",
    "\n",
    "def safe_display(obj):\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(obj)\n",
    "    except ImportError:\n",
    "        print(obj)\n",
    "\n",
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "\n",
    "print(f\"\\n1. Suspect Categories ({len(quality_report.get('category_issues', []))} found):\")\n",
    "safe_display(quality_report.get('category_issues', 'No issues found.'))\n",
    "\n",
    "print(f\"\\n2. Date Issues:\")\n",
    "date_issues = quality_report.get('date_issues', {})\n",
    "print(f\"- Range: {date_issues.get('date_range', ('N/A','N/A'))[0]} to {date_issues.get('date_range', ('N/A','N/A'))[1]}\")\n",
    "print(f\"- Missing dates: {len(date_issues.get('missing_dates', []) )}\")\n",
    "print(f\"- Pre-epidemic records: {len(date_issues.get('pre_epidemic_records', []) )}\")\n",
    "print(f\"- Duplicate dates: {date_issues.get('duplicate_dates', 0)}\")\n",
    "print(f\"- Inferred frequency: {date_issues.get('inferred_frequency', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n3. Demand Outliers (>{high_thresh:.1f} or <{low_thresh:.1f}):\")\n",
    "demand_out = quality_report.get('demand_outliers', {})\n",
    "print(f\"- {demand_out.get('outlier_count', 0)} extreme values\")\n",
    "print(f\"- Demand min: {demand_out.get('min', 'N/A')}, max: {demand_out.get('max', 'N/A')}\")\n",
    "\n",
    "print(\"\\n4. Missing Values by Column:\")\n",
    "safe_display(quality_report.get('missing_data', 'No missing values.'))\n",
    "print(f\"- Rows with any missing value: {quality_report.get('rows_with_any_missing', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65938f4",
   "metadata": {
    "papermill": {
     "duration": 0.003949,
     "end_time": "2025-06-05T14:19:01.815421",
     "exception": false,
     "start_time": "2025-06-05T14:19:01.811472",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ae6099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T14:19:01.825308Z",
     "iopub.status.busy": "2025-06-05T14:19:01.824993Z",
     "iopub.status.idle": "2025-06-05T14:19:01.885740Z",
     "shell.execute_reply": "2025-06-05T14:19:01.884560Z"
    },
    "papermill": {
     "duration": 0.067479,
     "end_time": "2025-06-05T14:19:01.887140",
     "exception": false,
     "start_time": "2025-06-05T14:19:01.819661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET COMPOSITION REPORT ===\n",
      "Unique SKUs:                       20\n",
      "Unique Categories:                 5\n",
      "Unique Stores:                     5\n",
      "SKUs with short history (<30):     0\n",
      "SKUs with zero total demand:       0\n",
      "\n",
      "Store Product Coverage (first 5):\n",
      "  Store S001: 20 SKUs\n",
      "  Store S002: 20 SKUs\n",
      "  Store S003: 20 SKUs\n",
      "  Store S004: 20 SKUs\n",
      "  Store S005: 20 SKUs\n",
      "\n",
      "SKU Reporting Frequency (first 5):\n",
      "  Irregular : 20 SKUs\n",
      "\n",
      "SKU Time Coverage (first 5):\n",
      "  SKU P0001: 2022-01-01 to 2024-01-30 (760 dates)\n",
      "  SKU P0002: 2022-01-01 to 2024-01-30 (760 dates)\n",
      "  SKU P0003: 2022-01-01 to 2024-01-30 (760 dates)\n",
      "  SKU P0004: 2022-01-01 to 2024-01-30 (760 dates)\n",
      "  SKU P0005: 2022-01-01 to 2024-01-30 (760 dates)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Retail Data Quality Audit â€“ Part B: Dataset Composition & Structure\n",
    "# ================================================================\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Setup logging for debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "composition_report = {}\n",
    "\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "        # 1. Product, Category, and Store Coverage\n",
    "        composition_report['n_unique_skus'] = df['Product ID'].nunique()\n",
    "        composition_report['n_unique_categories'] = df['Category'].nunique()\n",
    "        composition_report['n_unique_stores'] = df['Store ID'].nunique()\n",
    "\n",
    "        # SKUs with short history (<30 records)\n",
    "        sku_counts = df.groupby('Product ID', observed=True).size()\n",
    "        composition_report['sku_with_short_history'] = sku_counts[sku_counts < 30]\n",
    "        logging.info(f\"SKUs with short history: {composition_report['sku_with_short_history'].to_dict()}\")\n",
    "\n",
    "        # SKUs with zero total demand\n",
    "        sku_zero_demand = df.groupby('Product ID', observed=True)['Demand'].sum()\n",
    "        composition_report['sku_zero_demand'] = sku_zero_demand[sku_zero_demand == 0]\n",
    "        logging.info(f\"SKUs with zero demand: {composition_report['sku_zero_demand'].to_dict()}\")\n",
    "\n",
    "        # 2. Time Coverage per SKU\n",
    "        def get_time_coverage(x):\n",
    "            return pd.Series({\n",
    "                'start': x.index.min(),\n",
    "                'end': x.index.max(),\n",
    "                'n_dates': x.index.nunique()\n",
    "            })\n",
    "        sku_time_coverage = df.groupby('Product ID', observed=True).apply(get_time_coverage)\n",
    "        composition_report['sku_time_coverage'] = sku_time_coverage\n",
    "\n",
    "        # 3. Store/Region Representation\n",
    "        store_product_coverage = df.groupby('Store ID', observed=True)['Product ID'].nunique()\n",
    "        composition_report['store_product_coverage'] = store_product_coverage\n",
    "\n",
    "        # 4. Reporting Frequency per SKU\n",
    "        def infer_group_freq(x):\n",
    "            try:\n",
    "                freq = pd.infer_freq(x.index)\n",
    "                return freq if freq is not None else 'Irregular'\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Frequency inference failed for a group: {e}\")\n",
    "                return 'Error'\n",
    "        sku_reporting_freq = df.groupby('Product ID', observed=True).apply(infer_group_freq).value_counts()\n",
    "        composition_report['sku_reporting_freq'] = sku_reporting_freq\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in dataset composition audit: {e}\")\n",
    "\n",
    "# === FORMATTED DISPLAY RESULTS ===\n",
    "print(\"=== DATASET COMPOSITION REPORT ===\")\n",
    "print(f\"{'Unique SKUs:':35}{composition_report.get('n_unique_skus', 'N/A')}\")\n",
    "print(f\"{'Unique Categories:':35}{composition_report.get('n_unique_categories', 'N/A')}\")\n",
    "print(f\"{'Unique Stores:':35}{composition_report.get('n_unique_stores', 'N/A')}\")\n",
    "print(f\"{'SKUs with short history (<30):':35}{len(composition_report.get('sku_with_short_history', []))}\")\n",
    "print(f\"{'SKUs with zero total demand:':35}{len(composition_report.get('sku_zero_demand', []))}\")\n",
    "\n",
    "print(\"\\nStore Product Coverage (first 5):\")\n",
    "store_cov = composition_report.get('store_product_coverage', 'N/A')\n",
    "if isinstance(store_cov, str):\n",
    "    print(store_cov)\n",
    "else:\n",
    "    for idx, val in store_cov.head().items():\n",
    "        print(f\"  Store {idx}: {val} SKUs\")\n",
    "\n",
    "print(\"\\nSKU Reporting Frequency (first 5):\")\n",
    "sku_freq = composition_report.get('sku_reporting_freq', 'N/A')\n",
    "if isinstance(sku_freq, str):\n",
    "    print(sku_freq)\n",
    "else:\n",
    "    for freq, count in sku_freq.head().items():\n",
    "        print(f\"  {freq:<10}: {count} SKUs\")\n",
    "\n",
    "print(\"\\nSKU Time Coverage (first 5):\")\n",
    "sku_time_cov = composition_report.get('sku_time_coverage', 'N/A')\n",
    "if isinstance(sku_time_cov, str):\n",
    "    print(sku_time_cov)\n",
    "else:\n",
    "    for idx, row in sku_time_cov.head().iterrows():\n",
    "        print(f\"  SKU {idx}: {row['start'].date()} to {row['end'].date()} ({row['n_dates']} dates)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e15989",
   "metadata": {
    "papermill": {
     "duration": 0.004044,
     "end_time": "2025-06-05T14:19:01.895555",
     "exception": false,
     "start_time": "2025-06-05T14:19:01.891511",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7386309,
     "sourceId": 11895299,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.770888,
   "end_time": "2025-06-05T14:19:02.318677",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T14:18:53.547789",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
